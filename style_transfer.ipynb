{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer.ipynb",
      "provenance": [],
      "mount_file_id": "1nQemdyATZFsb45WPi-0AooX-YazS3J8s",
      "authorship_tag": "ABX9TyPr2L4UmXL8bbq2CK1ElmET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX160B-hEFGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import image as kp_image\n",
        "\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "tf.compat.v1.disable_v2_behavior()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB15q5wwWkWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess image\n",
        "def load_img(img_path):\n",
        "  img = Image.open(img_path)\n",
        "\n",
        "  # resize to max dimension\n",
        "  max_dim = 512\n",
        "  img_size = max(img.size)\n",
        "  scale = max_dim / img_size \n",
        "  img = img.resize((round(img.size[0] * scale), round(img.size[1] * scale)), Image.ANTIALIAS)\n",
        "\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # required step to run vgg19\n",
        "  out = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "\n",
        "  return out\n",
        "\n",
        "# postprocess image\n",
        "def postprocess_img(processed_img):\n",
        "  img = processed_img.copy()\n",
        "\n",
        "  img[:, :, 0] += 103.939\n",
        "  img[:, :, 1] += 116.779\n",
        "  img[:, :, 2] += 123.68\n",
        "\n",
        "  img = img[:, :, ::-1]\n",
        "\n",
        "  img = np.clip(img, 0, 255).astype('uint8')\n",
        "\n",
        "  return img"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE7hMxGxXCYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build loss functions from scratch\n",
        "\n",
        "def gram_matrix(input_tensor):\n",
        "  # 3D => 2D matrix: nh * nw * nc => nc * (nh * nw)\n",
        "  channels = int(input_tensor.shape[-1])\n",
        "  a = tf.reshape(input_tensor, [-1, channels])\n",
        "  n = tf.shape(a)[0]\n",
        "\n",
        "  gram = tf.matmul(a, a, transpose_a=True)\n",
        "\n",
        "  return gram\n",
        "\n",
        "def get_style_loss(base_style, gram_target): # base_style is generated layer output, gram_target is style layer output\n",
        "  height, width, channels = base_style.get_shape().as_list()\n",
        "\n",
        "  gram_style = gram_matrix(base_style)\n",
        "\n",
        "  return tf.reduce_mean(tf.square(gram_style - gram_target)) / (channels**2 * width * height) #(4.0 * (channels ** 2) * (width * height) ** 2)\n",
        "\n",
        "def get_content_loss(content, target): # content is new generated image, target is original image\n",
        "  return tf.reduce_mean(tf.square(content - target)) / 2\n",
        "\n",
        "def compute_loss(loss_weights, generated_output_activations, gram_style_features, content_features, num_content_layers, num_style_layers):\n",
        "  gen_style_features, gen_content_features = generated_output_activations\n",
        "\n",
        "  style_score = 0\n",
        "  content_score = 0\n",
        "\n",
        "  # accumulate style losses from all layers\n",
        "  # weight the same fore ach layer\n",
        "  weight_per_style_layer = 1.0 / float(num_style_layers)\n",
        "  style_score = get_total_style_loss(gen_style_features, gram_style_features)\n",
        "\n",
        "  weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "  content_score = get_total_content_loss(gen_content_features, content_features)\n",
        "\n",
        "  loss = style_weight * style_score + content_weight * content_score\n",
        "\n",
        "  return loss, style_score, content_score"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEb1p7PwVUZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get feature outputs for style and content images\n",
        "def get_feature_representations(model, content_img_path, style_img_path):\n",
        "\n",
        "  content_img = load_img(content_img_path)\n",
        "  style_img = load_img(style_img_path)\n",
        "\n",
        "  content_outputs = model.predict(content_img)[num_style_layers:] # only get content features of content image (dont care about style)\n",
        "  style_outputs = model.predict(style_img)[:num_style_layers] # only get style features of style image\n",
        "\n",
        "  content_features = [ content_layer[0] for content_layer in content_outputs ]\n",
        "  style_features = [ style_layer[0] for style_layer in style_outputs ]\n",
        "\n",
        "  return style_features, content_features"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWirJovvR1a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(loss_weights):\n",
        "  vgg19 = VGG19(weights=None, include_top=False)\n",
        "  vgg19.trainable = False # pretrained; don't touch\n",
        "\n",
        "  content_model_outputs = [vgg19.get_layer(layer).output for layer in content_layers]\n",
        "  style_model_outputs = [vgg19.get_layer(layer).output for layer in style_layers]\n",
        "\n",
        "  model_outputs = style_model_outputs + content_model_outputs # must combine output...we cannot output lists as outputs, we must have a list of outputs if anything\n",
        "\n",
        "  model = Model(inputs=vgg19.input, outputs=model_outputs)\n",
        "\n",
        "  style_weight, content_weight = loss_weights\n",
        "\n",
        "  losses = [get_style_loss for style_layer in style_layers] + [get_content_loss for content_layer in content_layers]\n",
        "  loss_weights = [style_weight for style_layer in style_layers] + [content_weight for content_layer in content_layers]\n",
        "\n",
        "  model.compile(loss=losses, loss_weights=loss_weights, optimizer=Adam())\n",
        "\n",
        "  vgg19.load_weights(vgg_weights_path)\n",
        "\n",
        "  return model, vgg19 # takes part of model up to the last output "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImZRuVjUniM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_style_transfer(num_iterations=200, content_weight=0.1, style_weight=0.9):\n",
        "  # sess = tf.compat.v1.Session()\n",
        "  # tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "  loss_weights = (style_weight, content_weight)\n",
        "\n",
        "  model, vgg19 = build_model(loss_weights)\n",
        "\n",
        "  # the loss is only updated if the current loss is better (i.e lesser) than the previous loss\n",
        "  best_loss, best_img = None, None\n",
        "\n",
        "  # get layer outputs from the content image and style image\n",
        "  style_features, content_features = get_feature_representations(model, content_img_path, style_img_path)\n",
        "\n",
        "  # get gram matrix values to prepare for total loss calc\n",
        "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "  # get layer outputs for generated image\n",
        "  generated_image = load_img(content_img_path)\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    history = model.fit(generated_image, gram_style_features + content_layers)\n",
        "    loss = history.history['loss']\n",
        "\n",
        "    if best_loss == None or loss < best_loss:\n",
        "      best_loss = loss\n",
        "      best_img = postprocess_img(generated_img)\n",
        "\n",
        "      print(f'best loss: {best_loss}')\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "      output = Image.fromarray(best_img)\n",
        "      output.save(os.path.join(project_path, f'{i + 1}-{save_img_path}'))\n",
        "\n",
        "      # save model\n",
        "      model.save_weights(os.path.join(project_path, 'style_transfer.weights'))\n",
        "\n",
        "  # how to use this?\n",
        "  # VGG default normalization\n",
        "  norm_means = np.array([103.939, 116.779, 123.68])\n",
        "  min_vals = -norm_means\n",
        "  max_vals = 255 - norm_means\n",
        "\n",
        "  return best_img, best_loss"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp-G9-MDr5O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of layers to caulcate for content and style loss\n",
        "content_layers = ['block3_conv3']\n",
        "style_layers = ['block1_conv1', 'block2_conv2', 'block4_conv3']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n",
        "\n",
        "project_path = '/content/drive/My Drive/hairy_gan'\n",
        "content_img_path = os.path.join(project_path, 'content.jpg')\n",
        "style_img_path = os.path.join(project_path, 'style.jpg')\n",
        "save_img_path = os.path.join(project_path, 'generated.jpg')\n",
        "\n",
        "vgg_weights_path = os.path.join(project_path, 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP18sz5NUo2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61511b71-f1e1-44a2-f51e-af30a68685e9"
      },
      "source": [
        "best, best_loss = run_style_transfer(content_img_path, style_img_path, save_img_path)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-db97f9004b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-a1ac23b84fc9>\u001b[0m in \u001b[0;36mrun_style_transfer\u001b[0;34m(num_iterations, content_weight, style_weight)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstyle_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# the loss is only updated if the current loss is better (i.e lesser) than the previous loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-6817cfc7181e>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(loss_weights)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mloss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstyle_weight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstyle_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstyle_layers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontent_weight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcontent_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mvgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[0;32m--> 692\u001b[0;31m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lambda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<lambda>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[1;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5c32b177bd70>\u001b[0m in \u001b[0;36mget_style_loss\u001b[0;34m(base_style, gram_target)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_style_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# base_style is generated layer output, gram_target is style layer output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mgram_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgram_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDhl2RnnUhxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff0acfb4-418c-46e7-e46a-f11458bdc8af"
      },
      "source": [
        "[1] + [2]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3OfrzJlXt3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}