{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fast_style_transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVj8dlSE5IIR+XpMNJ+WEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/fast_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2QUooqstwlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51b2d3a6-1d72-4776-8757-f78aeda407b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGY69RstsCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3b722cdf-cff1-4df2-fea9-7bf4e5e17d06"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "  \n",
        "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "  && cd keras-contrib \\\n",
        "  && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "  && python convert_to_tf_keras.py \\\n",
        "  && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-contrib' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8fRD2mfejoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7c78a5fa-5ad1-407c-bb49-2f36cd7d5387"
      },
      "source": [
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Embedding, Lambda\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, save_model\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.backend import set_session, clear_session\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "\n",
        "from keras.applications.vgg19 import preprocess_input, VGG19\n",
        "\n",
        "from keras.losses import mean_squared_error\n",
        "from keras import backend as k"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k62myyK0tZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://www.machinecurve.com/index.php/2020/02/10/using-constant-padding-reflection-padding-and-replication-padding-with-keras/\n",
        "from keras.layers import Layer\n",
        "\n",
        "'''\n",
        "  2D Reflection Padding\n",
        "  Attributes:\n",
        "    - padding: (padding_width, padding_height) tuple\n",
        "'''\n",
        "class ReflectionPadding2D(Layer):\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, s):\n",
        "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
        "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        w_pad,h_pad = self.padding\n",
        "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01lfdgevxLIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, stride):\n",
        "  reflection_size = kernel_size // 2\n",
        "  reflection_padding = (reflection_size, reflection_size)\n",
        "  x = ReflectionPadding2D(reflection_padding)(x)\n",
        "  x = Conv2D(filters=num_filters, kernel_size=kernel_size, strides=stride)(x)\n",
        "  return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLqVXU4V1Tpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x, num_filters):\n",
        "  input = x\n",
        "  x = conv_layer(x, num_filters, 3, 1)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "\n",
        "  x = conv_layer(x, num_filters, 3, 1)\n",
        "  x = InstanceNormalization()(x)\n",
        "\n",
        "  x = Concatenate(axis=-1)([input, x])\n",
        "  return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niMpdkan5zun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deconv_layer(x, num_filters, kernel_size, stride, upsample_size):\n",
        "  reflection_size = kernel_size // 2\n",
        "  reflection_padding = (reflection_size, reflection_size)\n",
        "  x = UpSampling2D(size=upsample_size)(x)\n",
        "  x = ReflectionPadding2D(reflection_padding)(x)\n",
        "  x = Conv2D(filters=num_filters, kernel_size=kernel_size, strides=stride)(x)\n",
        "  return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh4WqtQsyeSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_net(input):\n",
        "  x = conv_layer(input, 32, 9, 1) #conv1\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 64, 3, 2) # conv2\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 128, 3, 2) # conv3\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = residual_block(x, 128) # res1\n",
        "  x = residual_block(x, 128) # res2\n",
        "  x = residual_block(x, 128) # res3\n",
        "  x = residual_block(x, 128) # res4\n",
        "  x = residual_block(x, 128) # res5\n",
        "  x = deconv_layer(x, 64, 3, 1, 2) # deconv1\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = deconv_layer(x, 32, 3, 1, 2) # deconv2\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 3, 9, 1) # deconv3? apparently.\n",
        "\n",
        "  model = Model(input=input, output=x)\n",
        "  return model\n",
        "\n",
        "def style_model(net):\n",
        "  input_img = Input(shape=img_shape)\n",
        "  gen_img = net(input_img)\n",
        "  # lambda function to normalize\n",
        "  normalized_gen_img = Lambda(normalize_imgs)(gen_img)\n",
        "  gen_features = extractor(normalized_gen_img)  \n",
        "  gen_style_features = gen_features[:num_style_layers]\n",
        "  gen_gram_style_features = [Lambda(gram_matrix)(gen_style_feature) for gen_style_feature in gen_style_features]\n",
        "  gen_content_features = gen_features[num_style_layers:]\n",
        "  \n",
        "  gen_features = gen_gram_style_features + gen_content_features\n",
        "\n",
        "  model = Model(input=input_img, output=gen_features)\n",
        "\n",
        "  # losses = [get_style_loss for i in range(num_style_layers)] + [get_content_loss for i in range(num_content_layers)]\n",
        "  losses = ['mean_squared_error' for i in range(len(gen_features))]\n",
        "\n",
        "  weight_per_style_layer = style_weight / num_style_layers\n",
        "  weight_per_content_layer = content_weight / num_content_layers\n",
        "\n",
        "  loss_weights = [ weight_per_style_layer  * pair[1] for i, pair in enumerate(style_weights.items()) ] + [ weight_per_content_layer for i in range(num_content_layers) ]\n",
        "\n",
        "  model.compile(loss=losses, loss_weights=loss_weights, optimizer=Adam(learning_rate=0.00002))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdMo8CDhfKn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (img_shape[0], img_shape[1]))\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  img = tf.expand_dims(img, axis=0)\n",
        "  return img\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JazHYuVDjH6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extractor(layer_names, model):\n",
        "  outputs = [model.get_layer(name).output for name in layer_names]\n",
        "  model = Model(inputs=[vgg.input], outputs=outputs)\n",
        "  model.trainable = False\n",
        "  return model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2_bnG3gl2kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_imgs(imgs):\n",
        "  imgs = imgs * 255.\n",
        "  imgs = preprocess_input(imgs)\n",
        "  return imgs\n",
        "\n",
        "def normalize_imgs(imgs, batch_size=1):\n",
        "  mean = np.array([0.485, 0.456, 0.406]).reshape([batch_size, 1, 1, -1])\n",
        "  std = np.array([0.229, 0.224, 0.225]).reshape([batch_size, 1, 1, -1])\n",
        "  imgs = imgs / 255.\n",
        "  return (imgs - mean) / std"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl4SkGo6r3ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "  temp = tensor\n",
        "  batch_size, height, width, channels = temp.get_shape().as_list()\n",
        "  fun = tf.reshape(temp, [channels, height * width])\n",
        "  result = tf.matmul(fun, fun, transpose_a=True)\n",
        "  gram = tf.expand_dims(result, axis=0)\n",
        "  gram = gram / (1. * channels * height * width)\n",
        "  return gram"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z0lLMsm8R8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_style_loss(gen_style, target_style):\n",
        "#   # gen_style = gram_matrix(gen_style)\n",
        "#   return tf.keras.losses.MeanAbsoluteError()(target_style, gen_style)\n",
        "\n",
        "# def get_content_loss(gen_content, target_content):\n",
        "#   return tf.keras.losses.MeanAbsoluteError()(target_content, gen_content)\n",
        "\n",
        "# def get_total_loss(gen_features, target_features):\n",
        "#   target_style_features = target_features[:num_style_layers]\n",
        "#   target_content_features = target_features[num_style_layers:]\n",
        "\n",
        "#   gen_style_features = gen_features[:num_style_layers]  \n",
        "\n",
        "#   total_style_loss = sum([get_style_loss(gen_feature, target_feature) for gen_feature, target_feature in zip(gen_style_features, target_style_features)])\n",
        "\n",
        "#   gen_content_features = gen_features[num_style_layers:]\n",
        "#   total_content_loss = sum([get_content_loss(gen_feature, target_feature) for gen_feature, target_feature in zip(gen_content_features, target_content_features)])\n",
        "\n",
        "#   total_loss = style_weight * total_style_loss + content_weight * total_content_loss\n",
        "\n",
        "#   return total_loss, total_style_loss, total_content_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH7zK54K_e7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @tf.function()\n",
        "# def train_step(input_img):\n",
        "#   with tf.GradientTape() as tape: # auto calculates gradinets\n",
        "#     outputs = model(input_img)\n",
        "#     print(outputs)\n",
        "#     loss, _, _ = get_total_loss(outputs, input_features)\n",
        "\n",
        "#   grad = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "#   opt.apply_gradients([(grad, model.trainable_weights)])\n",
        "\n",
        "#   image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)) # clip pixels to be in range of [0, 1]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E49FGxF5Goh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vgg + style net setup\n",
        "vgg = VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False\n",
        "\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1','block5_conv1']\n",
        "style_weights = { \n",
        "    'block1_conv1': 1., \n",
        "    'block2_conv1': 0.8,\n",
        "    'block3_conv1': 0.5,\n",
        "    'block4_conv1': 0.3,\n",
        "    'block5_conv1': 0.1\n",
        "}\n",
        "content_layers = ['block4_conv2']\n",
        "\n",
        "num_style_layers = len(style_layers)\n",
        "num_content_layers = len(content_layers)\n",
        "\n",
        "extractor = feature_extractor(style_layers + content_layers, vgg)\n",
        "\n",
        "opt = tf.optimizers.Adam(learning_rate=0.02)\n",
        "\n",
        "# image setup\n",
        "img_shape = (128, 128, 3)\n",
        "project_path = '/content/drive/My Drive/hairy_gan/'\n",
        "style_img_path = os.path.join(project_path, 'style.jpg')\n",
        "input_img_path = os.path.join(project_path, 'content.jpg')\n",
        "input_img = load_image(input_img_path)\n",
        "style_img = load_image(style_img_path)\n",
        "\n",
        "processed_style_img = preprocess_imgs(style_img)\n",
        "normalize_style_img = normalize_imgs(processed_style_img)\n",
        "style_features = extractor(normalize_style_img)[:num_style_layers]\n",
        "gram_style_features = [gram_matrix(feature) for feature in style_features]\n",
        "\n",
        "processed_input_img = preprocess_imgs(input_img) # x\n",
        "normalize_input_img = normalize_imgs(processed_input_img)\n",
        "input_content_features = extractor(normalize_input_img)[num_style_layers:] # features_x\n",
        "target_img = processed_input_img"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYB2RxEBsWa-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bc38e06-8212-450f-8acb-088f351e4126"
      },
      "source": [
        "epochs = 100\n",
        "steps_per_epoch = 1\n",
        "content_weight = 10\n",
        "style_weight = 100\n",
        "\n",
        "input = Input(shape=img_shape)\n",
        "net = style_net(input)\n",
        "model = style_model(net)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoAcP-OCr7jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "10d7041e-67f9-4c10-a45a-b41fe4a3a5a0"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  model.fit(target_img, gram_style_features + input_content_features, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1/1 [==============================] - 14s 14s/step - loss: 9134.1191 - lambda_27_loss: 7.0468e-09 - lambda_28_loss: 1.8164e-04 - lambda_29_loss: 0.0042 - lambda_30_loss: 14.6896 - lambda_31_loss: 0.0069 - model_18_loss: 904.5924\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 9113.2812 - lambda_27_loss: 7.1807e-09 - lambda_28_loss: 1.8187e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6847 - lambda_31_loss: 0.0069 - model_18_loss: 902.5115\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 9091.6670 - lambda_27_loss: 7.2546e-09 - lambda_28_loss: 1.8199e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6770 - lambda_31_loss: 0.0069 - model_18_loss: 900.3547\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 9054.8486 - lambda_27_loss: 7.2901e-09 - lambda_28_loss: 1.8201e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6671 - lambda_31_loss: 0.0069 - model_18_loss: 896.6788\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 9001.9150 - lambda_27_loss: 7.2899e-09 - lambda_28_loss: 1.8193e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6516 - lambda_31_loss: 0.0069 - model_18_loss: 891.3948\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8946.5488 - lambda_27_loss: 7.2604e-09 - lambda_28_loss: 1.8178e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6329 - lambda_31_loss: 0.0069 - model_18_loss: 885.8693\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8896.5977 - lambda_27_loss: 7.2070e-09 - lambda_28_loss: 1.8158e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6156 - lambda_31_loss: 0.0068 - model_18_loss: 880.8846\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8846.6504 - lambda_27_loss: 7.1467e-09 - lambda_28_loss: 1.8138e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.6000 - lambda_31_loss: 0.0068 - model_18_loss: 875.8992\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8792.8457 - lambda_27_loss: 7.0983e-09 - lambda_28_loss: 1.8118e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.5843 - lambda_31_loss: 0.0068 - model_18_loss: 870.5282\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8739.8145 - lambda_27_loss: 7.0688e-09 - lambda_28_loss: 1.8103e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.5695 - lambda_31_loss: 0.0068 - model_18_loss: 865.2339\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8689.4365 - lambda_27_loss: 7.0587e-09 - lambda_28_loss: 1.8091e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.5570 - lambda_31_loss: 0.0068 - model_18_loss: 860.2037\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8643.0752 - lambda_27_loss: 7.0612e-09 - lambda_28_loss: 1.8083e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.5471 - lambda_31_loss: 0.0067 - model_18_loss: 855.5735\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8600.6377 - lambda_27_loss: 7.0666e-09 - lambda_28_loss: 1.8075e-04 - lambda_29_loss: 0.0041 - lambda_30_loss: 14.5393 - lambda_31_loss: 0.0067 - model_18_loss: 851.3345\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5mXEwaOkWO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "4a57602c-2146-4cbb-a797-c4fc860e1c83"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_13 (Model)                (None, 128, 128, 3)  3519265     input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 128, 128, 3)  0           model_13[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "model_12 (Model)                multiple             12944960    lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (1, 16384, 16384)    0           model_12[3][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (1, 4096, 4096)      0           model_12[3][1]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (1, 1024, 1024)      0           model_12[3][2]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (1, 256, 256)        0           model_12[3][3]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (1, 64, 64)          0           model_12[3][4]                   \n",
            "==================================================================================================\n",
            "Total params: 16,464,225\n",
            "Trainable params: 16,464,225\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYl4wqR2r8DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}