{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fast_style_transfer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeOWNptqbpuG6IR1AJ4G1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/fast_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2QUooqstwlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b53e75e-77ec-4ea0-9557-b76d21b8e0f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGY69RstsCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9c388f3c-1eea-4556-a8a5-f3f12ac13129"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "  \n",
        "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "  && cd keras-contrib \\\n",
        "  && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "  && python convert_to_tf_keras.py \\\n",
        "  && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-contrib' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8fRD2mfejoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d0fec57d-ad7f-4be4-a3e0-0a0bb0b3f18f"
      },
      "source": [
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Embedding, Lambda\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, save_model\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.backend import set_session, clear_session\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "\n",
        "from keras.applications.vgg19 import preprocess_input, VGG19\n",
        "\n",
        "from keras.losses import mean_squared_error\n",
        "from keras import backend as k"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k62myyK0tZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://www.machinecurve.com/index.php/2020/02/10/using-constant-padding-reflection-padding-and-replication-padding-with-keras/\n",
        "from keras.layers import Layer\n",
        "\n",
        "'''\n",
        "  2D Reflection Padding\n",
        "  Attributes:\n",
        "    - padding: (padding_width, padding_height) tuple\n",
        "'''\n",
        "class ReflectionPadding2D(Layer):\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, s):\n",
        "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
        "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        w_pad,h_pad = self.padding\n",
        "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01lfdgevxLIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(x, num_filters, kernel_size, stride):\n",
        "  reflection_size = kernel_size // 2\n",
        "  reflection_padding = (reflection_size, reflection_size)\n",
        "  x = ReflectionPadding2D(reflection_padding)(x)\n",
        "  x = Conv2D(filters=num_filters, kernel_size=kernel_size, strides=stride)(x)\n",
        "  return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLqVXU4V1Tpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x, num_filters):\n",
        "  input = x\n",
        "  x = conv_layer(x, num_filters, 3, 1)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "\n",
        "  x = conv_layer(x, num_filters, 3, 1)\n",
        "  x = InstanceNormalization()(x)\n",
        "\n",
        "  x = Concatenate(axis=-1)([input, x])\n",
        "  return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niMpdkan5zun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deconv_layer(x, num_filters, kernel_size, stride, upsample_size):\n",
        "  reflection_size = kernel_size // 2\n",
        "  reflection_padding = (reflection_size, reflection_size)\n",
        "  x = UpSampling2D(size=upsample_size)(x)\n",
        "  x = ReflectionPadding2D(reflection_padding)(x)\n",
        "  x = Conv2D(filters=num_filters, kernel_size=kernel_size, strides=stride)(x)\n",
        "  return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh4WqtQsyeSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_net(input):\n",
        "  x = conv_layer(input, 32, 9, 1) #conv1\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 64, 3, 2) # conv2\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 128, 3, 2) # conv3\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = residual_block(x, 128) # res1\n",
        "  x = residual_block(x, 128) # res2\n",
        "  x = residual_block(x, 128) # res3\n",
        "  x = residual_block(x, 128) # res4\n",
        "  x = residual_block(x, 128) # res5\n",
        "  x = deconv_layer(x, 64, 3, 1, 2) # deconv1\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = deconv_layer(x, 32, 3, 1, 2) # deconv2\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = conv_layer(x, 3, 9, 1) # deconv3? apparently.\n",
        "\n",
        "  model = Model(input=input, output=x)\n",
        "  return model\n",
        "\n",
        "def style_model(net):\n",
        "  input_img = Input(shape=img_shape)\n",
        "  gen_img = net(input_img)\n",
        "\n",
        "  gen_features = extractor(gen_img)  \n",
        "  gen_style_features = gen_features[:num_style_layers]\n",
        "  gen_gram_style_features = [Lambda(gram_matrix)(gen_style_feature) for gen_style_feature in gen_style_features]\n",
        "  gen_content_features = gen_features[num_style_layers:]\n",
        "  \n",
        "  gen_features = gen_gram_style_features + gen_content_features\n",
        "\n",
        "  model = Model(input=input_img, output=gen_features)\n",
        "\n",
        "  # losses = [get_style_loss for i in range(num_style_layers)] + [get_content_loss for i in range(num_content_layers)]\n",
        "  losses = ['mean_squared_error' for i in range(len(gen_features))]\n",
        "\n",
        "  weight_per_style_layer = style_weight / num_style_layers\n",
        "  weight_per_content_layer = content_weight / num_content_layers\n",
        "\n",
        "  loss_weights = [ weight_per_style_layer for i in range(num_style_layers) ] + [ weight_per_content_layer for i in range(num_content_layers) ]\n",
        "\n",
        "  model.compile(loss=losses, loss_weights=loss_weights, optimizer=Adam())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdMo8CDhfKn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(img_path, img_type='normal'):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (img_shape[0], img_shape[1]))\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  return img\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JazHYuVDjH6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extractor(layer_names, model):\n",
        "  outputs = [model.get_layer(name).output for name in layer_names]\n",
        "  model = Model(inputs=[vgg.input], outputs=outputs)\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2_bnG3gl2kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_imgs(imgs):\n",
        "  # imgs = preprocess_input(imgs)\n",
        "  imgs = imgs / 255.\n",
        "  return imgs"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl4SkGo6r3ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "  temp = tensor\n",
        "  batch_size, height, width, channels = temp.shape\n",
        "  fun = tf.reshape(temp, [channels, height * width])\n",
        "  result = tf.matmul(fun, fun, transpose_a=True)\n",
        "  gram = tf.expand_dims(result, axis=0)\n",
        "  return gram"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z0lLMsm8R8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_style_loss(gen_style, target_style):\n",
        "#   # gen_style = gram_matrix(gen_style)\n",
        "#   return tf.keras.losses.MeanAbsoluteError()(target_style, gen_style)\n",
        "\n",
        "# def get_content_loss(gen_content, target_content):\n",
        "#   return tf.keras.losses.MeanAbsoluteError()(target_content, gen_content)\n",
        "\n",
        "# def get_total_loss(gen_features, target_features):\n",
        "#   target_style_features = target_features[:num_style_layers]\n",
        "#   target_content_features = target_features[num_style_layers:]\n",
        "\n",
        "#   gen_style_features = gen_features[:num_style_layers]  \n",
        "\n",
        "#   total_style_loss = sum([get_style_loss(gen_feature, target_feature) for gen_feature, target_feature in zip(gen_style_features, target_style_features)])\n",
        "\n",
        "#   gen_content_features = gen_features[num_style_layers:]\n",
        "#   total_content_loss = sum([get_content_loss(gen_feature, target_feature) for gen_feature, target_feature in zip(gen_content_features, target_content_features)])\n",
        "\n",
        "#   total_loss = style_weight * total_style_loss + content_weight * total_content_loss\n",
        "\n",
        "#   return total_loss, total_style_loss, total_content_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH7zK54K_e7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @tf.function()\n",
        "# def train_step(input_img):\n",
        "#   with tf.GradientTape() as tape: # auto calculates gradinets\n",
        "#     outputs = model(input_img)\n",
        "#     print(outputs)\n",
        "#     loss, _, _ = get_total_loss(outputs, input_features)\n",
        "\n",
        "#   grad = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "#   opt.apply_gradients([(grad, model.trainable_weights)])\n",
        "\n",
        "#   image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)) # clip pixels to be in range of [0, 1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E49FGxF5Goh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfc0cf3c-2f0a-40ed-b19a-0299784aa6e5"
      },
      "source": [
        "img_shape = (128, 128, 3)\n",
        "project_path = '/content/drive/My Drive/hairy_gan/'\n",
        "style_img_path = os.path.join(project_path, 'style.jpg')\n",
        "input_img_path = os.path.join(project_path, 'content.jpg')\n",
        "input_img = load_image(input_img_path)\n",
        "input_img = tf.expand_dims(input_img, axis=0)\n",
        "style_img = load_image(style_img_path)\n",
        "style_img = tf.expand_dims(style_img, axis=0)\n",
        "\n",
        "vgg = VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False\n",
        "\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1','block5_conv1']\n",
        "content_layers = ['block4_conv2']\n",
        "\n",
        "num_style_layers = len(style_layers)\n",
        "num_content_layers = len(content_layers)\n",
        "\n",
        "extractor = feature_extractor(style_layers + content_layers, vgg)\n",
        "\n",
        "input = Input(shape=img_shape)\n",
        "net = style_net(input)\n",
        "opt = tf.optimizers.Adam(learning_rate=0.02)\n",
        "\n",
        "processed_style_img = preprocess_imgs(style_img)\n",
        "style_features = extractor(processed_style_img)[:num_style_layers]\n",
        "gram_style_features = [gram_matrix(feature) for feature in style_features]\n",
        "\n",
        "processed_input_img = preprocess_imgs(input_img)\n",
        "input_features = extractor(processed_input_img)\n",
        "input_content_features = input_features[num_style_layers:]\n",
        "# input_features = gram_style_features + input_content_features"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYB2RxEBsWa-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a85c0aa-cf23-4572-8d39-a9d84eb55269"
      },
      "source": [
        "epochs = 100\n",
        "steps_per_epoch = 1\n",
        "content_weight = 10\n",
        "style_weight = 100\n",
        "\n",
        "model = style_model(net)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF88Cxff-Vf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "cead97d7-6b6d-4f47-9cba-60ad293b469f"
      },
      "source": [
        "sess = tf.compat.v1.Session()\n",
        "og_img = None\n",
        "with sess:\n",
        "  og_img = processed_input_img.eval()\n",
        "\n",
        "# print(net.predict(og_img))\n",
        "og_img"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.00239908, 0.00256824, 0.00299885],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         ...,\n",
              "         [0.00244521, 0.00266052, 0.00306036],\n",
              "         [0.00242983, 0.00264514, 0.00304498],\n",
              "         [0.00242983, 0.00264514, 0.00304498]],\n",
              "\n",
              "        [[0.00239908, 0.00256824, 0.00299885],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         ...,\n",
              "         [0.00244521, 0.00266052, 0.00306036],\n",
              "         [0.00242983, 0.00264514, 0.00304498],\n",
              "         [0.00241446, 0.00262976, 0.0030296 ]],\n",
              "\n",
              "        [[0.00241446, 0.00258362, 0.00301423],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         [0.00241446, 0.00258362, 0.00301423],\n",
              "         ...,\n",
              "         [0.00241446, 0.00262976, 0.0030296 ],\n",
              "         [0.00239908, 0.00261438, 0.00301423],\n",
              "         [0.00239908, 0.00261438, 0.00301423]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.00321415, 0.00266052, 0.00192234],\n",
              "         [0.00292195, 0.00235294, 0.00167628],\n",
              "         [0.00312188, 0.00258362, 0.00193772],\n",
              "         ...,\n",
              "         [0.00210688, 0.00164552, 0.00086121],\n",
              "         [0.00336794, 0.00293733, 0.00224529],\n",
              "         [0.00270665, 0.00215302, 0.0013687 ]],\n",
              "\n",
              "        [[0.0031065 , 0.00255286, 0.00178393],\n",
              "         [0.00315263, 0.00261438, 0.00196847],\n",
              "         [0.0030296 , 0.00244521, 0.00184544],\n",
              "         ...,\n",
              "         [0.00272203, 0.00219915, 0.00143022],\n",
              "         [0.00327566, 0.00278354, 0.00199923],\n",
              "         [0.00262976, 0.00215302, 0.00139946]],\n",
              "\n",
              "        [[0.00295271, 0.00239908, 0.00161476],\n",
              "         [0.00313726, 0.00258362, 0.00193772],\n",
              "         [0.00318339, 0.00264514, 0.00202999],\n",
              "         ...,\n",
              "         [0.00326028, 0.00275279, 0.00196847],\n",
              "         [0.00312188, 0.00262976, 0.00184544],\n",
              "         [0.00284506, 0.00235294, 0.00153787]]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoAcP-OCr7jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "e3d9f84a-1801-47b1-a805-f7b8b5d29c9a"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    model.fit(processed_input_img, gram_style_features + input_content_features, steps_per_epoch=steps_per_epoch)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1/1 [==============================] - 11s 11s/step - loss: 299152769024.0000 - lambda_36_loss: 5051.5137 - lambda_37_loss: 1658547.5000 - lambda_38_loss: 10865487.0000 - lambda_39_loss: 14944248832.0000 - lambda_40_loss: 860154.1250 - model_22_loss: 214.3780\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 8s 8s/step - loss: 138493591552.0000 - lambda_36_loss: 124603.3281 - lambda_37_loss: 16608171.0000 - lambda_38_loss: 49491200.0000 - lambda_39_loss: 6858442752.0000 - lambda_40_loss: 12934.1914 - model_22_loss: 86.8002\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 121738048.0000 - lambda_36_loss: 16010.6006 - lambda_37_loss: 1842170.2500 - lambda_38_loss: 2762257.0000 - lambda_39_loss: 1466463.3750 - lambda_40_loss: 0.4453 - model_22_loss: 0.9255\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 77104368.0000 - lambda_36_loss: 13675.6523 - lambda_37_loss: 1738313.0000 - lambda_38_loss: 2061468.7500 - lambda_39_loss: 41761.0234 - lambda_40_loss: 4.0337e-04 - model_22_loss: 0.1091\n",
            "Epoch 1/1\n",
            "1/1 [==============================] - 7s 7s/step - loss: 139640128.0000 - lambda_36_loss: 19571.8652 - lambda_37_loss: 3285802.5000 - lambda_38_loss: 3615708.7500 - lambda_39_loss: 60922.8828 - lambda_40_loss: 0.0046 - model_22_loss: 0.0732\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-7552fe3a94c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_input_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_style_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_content_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3632\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forSCak5FZBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}