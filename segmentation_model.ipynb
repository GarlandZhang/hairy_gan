{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I61kYxdeY_e-y7_7sNtBKbCVJMvoqLJ0",
      "authorship_tag": "ABX9TyPWJQVU50/nHDxUeuqVteuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/segmentation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmArMn_FGXpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "project_path = '/content/drive/My Drive/hairy_gan/'\n",
        "dataset_path = os.path.join(project_path, 'face_segment')\n",
        "src_img_target_path = os.path.join(dataset_path, 'trainA')\n",
        "label_img_target_path = os.path.join(dataset_path, 'trainB')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkczVm_WBHyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move images and corresponding segmentation masks into their own folders\n",
        "# tree = ET.parse(os.path.join(dataset_path, 'sampleset.xml'))\n",
        "# root = tree.getroot()\n",
        "# children = root.getchildren()\n",
        "# for i in range(len(children) // 2):\n",
        "#   src_img_path = children[2 * i].get('name').replace('\\\\', '/')\n",
        "#   label_img_path = children[2 * i + 1].get('name').replace('\\\\', '/')\n",
        "  \n",
        "#   shutil.copy(os.path.join(dataset_path, src_img_path), os.path.join(src_img_target_path, os.path.basename(src_img_path)))\n",
        "#   shutil.copy(os.path.join(dataset_path, label_img_path), os.path.join(label_img_target_path, os.path.basename(label_img_path)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2WmmlCzDYKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e676a3d4-f537-4fce-8f3d-4655d0068276"
      },
      "source": [
        "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "!pip install pillow\n",
        "!pip install scipy==1.1.0        "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-contrib'...\n",
            "warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "remote: Enumerating objects: 3634, done.\u001b[K\n",
            "remote: Total 3634 (delta 0), reused 0 (delta 0), pack-reused 3634\u001b[K\n",
            "Receiving objects: 100% (3634/3634), 861.24 KiB | 7.62 MiB/s, done.\n",
            "Resolving deltas: 100% (2330/2330), done.\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-6aykkelv\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-6aykkelv\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=6809300c959f2b7f62785a99f8387fa8a3c62ca7c855149bb6f1ab2922239a5b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pqetduua/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Changed imports in 73 files.\n",
            "Those files were found in the directory /content/keras-contrib\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating tf_keras_contrib.egg-info\n",
            "writing tf_keras_contrib.egg-info/PKG-INFO\n",
            "writing dependency_links to tf_keras_contrib.egg-info/dependency_links.txt\n",
            "writing requirements to tf_keras_contrib.egg-info/requires.txt\n",
            "writing top-level names to tf_keras_contrib.egg-info/top_level.txt\n",
            "writing manifest file 'tf_keras_contrib.egg-info/SOURCES.txt'\n",
            "writing manifest file 'tf_keras_contrib.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/keras_contrib\n",
            "copying keras_contrib/__init__.py -> build/lib/keras_contrib\n",
            "creating build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/densenet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/wide_resnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/__init__.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/nasnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/resnet.py -> build/lib/keras_contrib/applications\n",
            "creating build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/cntk_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/__init__.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/numpy_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/theano_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/tensorflow_backend.py -> build/lib/keras_contrib/backend\n",
            "creating build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/__init__.py -> build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/clip.py -> build/lib/keras_contrib/constraints\n",
            "creating build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/__init__.py -> build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/convaware.py -> build/lib/keras_contrib/initializers\n",
            "creating build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/__init__.py -> build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/crf_accuracies.py -> build/lib/keras_contrib/metrics\n",
            "creating build/lib/keras_contrib/wrappers\n",
            "copying keras_contrib/wrappers/__init__.py -> build/lib/keras_contrib/wrappers\n",
            "creating build/lib/keras_contrib/regularizers\n",
            "copying keras_contrib/regularizers/__init__.py -> build/lib/keras_contrib/regularizers\n",
            "creating build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/core.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/__init__.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/crf.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/capsule.py -> build/lib/keras_contrib/layers\n",
            "creating build/lib/keras_contrib/preprocessing\n",
            "copying keras_contrib/preprocessing/__init__.py -> build/lib/keras_contrib/preprocessing\n",
            "creating build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/conv_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/__init__.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/save_load_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/test_utils.py -> build/lib/keras_contrib/utils\n",
            "creating build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/coco.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/__init__.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/conll2000.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/pascal_voc.py -> build/lib/keras_contrib/datasets\n",
            "creating build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/__init__.py -> build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/squash.py -> build/lib/keras_contrib/activations\n",
            "creating build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/optimizers.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/__init__.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/activations.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/regularizers.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/metrics.py -> build/lib/keras_contrib/tests\n",
            "creating build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/__init__.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/lars.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/yogi.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/padam.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/ftml.py -> build/lib/keras_contrib/optimizers\n",
            "creating build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/crf_losses.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/dssim.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/__init__.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/jaccard.py -> build/lib/keras_contrib/losses\n",
            "creating build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/dead_relu_detector.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/tensorboard.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/__init__.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/cyclical_learning_rate.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/snapshot.py -> build/lib/keras_contrib/callbacks\n",
            "creating build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/instancenormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/__init__.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/groupnormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "creating build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/__init__.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/subpixelupscaling.py -> build/lib/keras_contrib/layers/convolutional\n",
            "creating build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/swish.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/sinerelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/__init__.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/srelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/pelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/densenet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/wide_resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/nasnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/cntk_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/numpy_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/theano_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/tensorflow_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/clip.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/convaware.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/crf_accuracies.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "copying build/lib/keras_contrib/wrappers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "copying build/lib/keras_contrib/regularizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/core.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/instancenormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/groupnormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/subpixelupscaling.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/crf.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/swish.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/sinerelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/srelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/pelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/capsule.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "copying build/lib/keras_contrib/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/conv_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/save_load_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/test_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/coco.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/conll2000.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/pascal_voc.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/squash.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/optimizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/activations.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/regularizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/metrics.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/lars.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/yogi.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/padam.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/ftml.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/crf_losses.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/dssim.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/jaccard.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/dead_relu_detector.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/tensorboard.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/cyclical_learning_rate.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/snapshot.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/densenet.py to densenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/wide_resnet.py to wide_resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/nasnet.py to nasnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/resnet.py to resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/cntk_backend.py to cntk_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/numpy_backend.py to numpy_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/theano_backend.py to theano_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/tensorflow_backend.py to tensorflow_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/clip.py to clip.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/convaware.py to convaware.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/crf_accuracies.py to crf_accuracies.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/wrappers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/regularizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/core.py to core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/instancenormalization.py to instancenormalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/groupnormalization.py to groupnormalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/cosineconvolution2d.py to cosineconvolution2d.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/subpixelupscaling.py to subpixelupscaling.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/crf.py to crf.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/swish.py to swish.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/sinerelu.py to sinerelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/srelu.py to srelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/pelu.py to pelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/capsule.py to capsule.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/preprocessing/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/conv_utils.py to conv_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/save_load_utils.py to save_load_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/test_utils.py to test_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/coco.py to coco.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/conll2000.py to conll2000.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/pascal_voc.py to pascal_voc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/squash.py to squash.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/optimizers.py to optimizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/activations.py to activations.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/regularizers.py to regularizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/lars.py to lars.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/yogi.py to yogi.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/padam.py to padam.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/ftml.py to ftml.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/crf_losses.py to crf_losses.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/dssim.py to dssim.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/jaccard.py to jaccard.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/dead_relu_detector.py to dead_relu_detector.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/tensorboard.py to tensorboard.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/cyclical_learning_rate.py to cyclical_learning_rate.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/snapshot.py to snapshot.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/tf_keras_contrib-2.0.8-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing tf_keras_contrib-2.0.8-py3.6.egg\n",
            "Copying tf_keras_contrib-2.0.8-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding tf-keras-contrib 2.0.8 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/tf_keras_contrib-2.0.8-py3.6.egg\n",
            "Processing dependencies for tf-keras-contrib==2.0.8\n",
            "Finished processing dependencies for tf-keras-contrib==2.0.8\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 94kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QsonbJdyaye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7edc8c28-ba6c-498d-a681-dedf8b1e0f2e"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import shutil\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import PIL\n",
        "from glob import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqcAQNxPGMQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_path, img_res=(128, 128)):\n",
        "      self.dataset_path = dataset_path\n",
        "      self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "      # data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "      data_type = 'train' + domain # always grab from training since we have no test\n",
        "      path = glob(os.path.join(self.dataset_path, data_type, '*'))\n",
        "\n",
        "      batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "      imgs = []\n",
        "      for img_path in batch_images:\n",
        "          img = self.imread(img_path)\n",
        "          if not is_testing:\n",
        "              img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "              if np.random.random() > 0.5:\n",
        "                  img = np.fliplr(img)\n",
        "          else:\n",
        "              img = scipy.misc.imresize(img, self.img_res)\n",
        "          imgs.append(img)\n",
        "\n",
        "      imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "      return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "      data_type = \"train\" if not is_testing else \"val\"\n",
        "      path_I = glob(os.path.join(self.dataset_path, data_type + 'I', '*'))\n",
        "      path_S = glob(os.path.join(self.dataset_path, data_type + 'S', '*'))\n",
        "\n",
        "      self.n_batches = int(min(len(path_I), len(path_S)) / batch_size)\n",
        "      total_samples = self.n_batches * batch_size\n",
        "\n",
        "      # Sample n_batches * batch_size from each path list so that model sees all\n",
        "      # samples from both domains\n",
        "      path_I = np.random.choice(path_I, total_samples, replace=False)\n",
        "      path_S = np.random.choice(path_S, total_samples, replace=False)\n",
        "\n",
        "      i = 0\n",
        "      while True:\n",
        "          batch_I = path_I[i*batch_size:(i+1)*batch_size]\n",
        "          batch_S = path_S[i*batch_size:(i+1)*batch_size]\n",
        "          imgs_I, imgs_S = [], []\n",
        "          for img_I, img_S in zip(batch_I, batch_S):\n",
        "              img_I = self.imread(img_I)\n",
        "              img_S = self.imread(img_S)\n",
        "\n",
        "              img_I = scipy.misc.imresize(img_I, self.img_res)\n",
        "              img_S = scipy.misc.imresize(img_S, self.img_res)\n",
        "\n",
        "              if not is_testing and np.random.random() > 0.5:\n",
        "                      img_I = np.fliplr(img_I)\n",
        "                      img_S = np.fliplr(img_S)\n",
        "\n",
        "              imgs_I.append(img_I)\n",
        "              imgs_S.append(img_S)\n",
        "\n",
        "          imgs_I = np.array(imgs_I)/127.5 - 1.\n",
        "          imgs_S = np.array(imgs_S)/127.5 - 1.\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          if i == self.n_batches - 1:\n",
        "            path_I = np.random.choice(path_I, total_samples, replace=False)\n",
        "            path_S = np.random.choice(path_S, total_samples, replace=False)\n",
        "            i = 0\n",
        "          yield imgs_I, imgs_S\n",
        "\n",
        "    def imread(self, path):\n",
        "      return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7awGdclIMOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SegGAN():\n",
        "  def __init__(self):\n",
        "    self.img_rows = 128\n",
        "    self.img_cols = 128\n",
        "    self.img_channels = 3\n",
        "\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "\n",
        "    self.data_loader = DataLoader(dataset_path, img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "    # patch = int(self.img_rows / 2**4)\n",
        "    # self.disc_patch = (patch, patch, 1) # output shape of discriminator\n",
        "\n",
        "    self.gf = 32 # num filters in first layer of gen\n",
        "    self.df = 64 # num filters in first layer of disc\n",
        "\n",
        "    optimizer = Adam(0.0002, 0.5, 0.999)\n",
        "\n",
        "    self.d_I = self.build_discriminator()\n",
        "    self.d_S = self.build_discriminator()\n",
        "\n",
        "    self.d_I.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "    self.d_S.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    self.d_output_shape = tuple(self.d_I.layers[-1].output.get_shape().as_list()[1:])\n",
        "\n",
        "    self.g_IS = self.build_generator()\n",
        "    self.g_SI = self.build_generator()\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'd_I.weights')):\n",
        "      self.d_I.load_weights(os.path.join(project_path, 'd_I.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'd_S.weights')):\n",
        "      self.d_S.load_weights(os.path.join(project_path, 'd_S.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'g_IS.weights')):\n",
        "      self.g_IS.load_weights(os.path.join(project_path, 'g_IS.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'g_SI.weights')):\n",
        "      self.g_SI.load_weights(os.path.join(project_path, 'g_SI.weights'))\n",
        "\n",
        "    img_I = Input(shape=self.img_shape)\n",
        "    img_S = Input(shape=self.img_shape)\n",
        "\n",
        "    fake_S = self.g_IS(img_I)\n",
        "    fake_I = self.g_SI(img_S)\n",
        "\n",
        "    reconstr_I = self.g_SI(fake_S)\n",
        "    reconstr_S = self.g_IS(fake_I)\n",
        "\n",
        "    valid_I = self.d_I(fake_I)\n",
        "    valid_S = self.d_S(fake_S)\n",
        "\n",
        "    self.combined = Model(inputs=[img_I, img_S], outputs=[valid_I, valid_S, fake_S, fake_I, reconstr_I, reconstr_S])\n",
        "    self.combined.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], loss_weights=[1, 1, 1, 1, 0.05, 1], optimizer=optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "    '''U-net generator'''\n",
        "\n",
        "    def conv2d(layer_input, filters, f_size=4):\n",
        "      '''Layers used during downsampling'''\n",
        "      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      d = InstanceNormalization()(d)\n",
        "      return d\n",
        "\n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "      '''Layers used during upsampling'''\n",
        "      u = UpSampling2D(size=2)(layer_input)\n",
        "      u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "\n",
        "      if dropout_rate:\n",
        "        u = Dropout(dropout_rate)(u)\n",
        "      u = InstanceNormalization()(u)\n",
        "      u = Concatenate()([u, skip_input])\n",
        "      return u\n",
        "\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = conv2d(d0, self.gf)\n",
        "    d2 = conv2d(d1, self.gf * 2)\n",
        "    d3 = conv2d(d2, self.gf * 4)\n",
        "    d4 = conv2d(d3, self.gf * 8)\n",
        "\n",
        "    u1 = deconv2d(d4, d3, self.gf * 4)\n",
        "    u2 = deconv2d(u1, d2, self.gf * 2)\n",
        "    u3 = deconv2d(u2, d1, self.gf)\n",
        "\n",
        "    u4 = UpSampling2D(size=2)(u3)\n",
        "    output_img = Conv2D(self.img_channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "    return Model(d0, output_img)\n",
        "\n",
        "  def build_discriminator(self):\n",
        "  \n",
        "    input = Input(shape=self.img_shape)\n",
        "    d1 = Conv2D(self.df, kernel_size=1, strides=1, padding='same')(input)\n",
        "    d2 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d3 = Conv2D(self.df * 2, kernel_size=1, strides=1, padding='same')(d2)\n",
        "    d4 = BatchNormalization()(d3)\n",
        "    d5 = LeakyReLU(alpha=0.2)(d4)\n",
        "    d6 = Conv2D(self.df * 2, kernel_size=1, strides=1, padding='same', activation='sigmoid')(d5)\n",
        "\n",
        "    return Model(input, d6)\n",
        "\n",
        "  def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    valid = np.ones((batch_size, ) + self.d_output_shape)\n",
        "\n",
        "    fake = np.zeros((batch_size, ) + self.d_output_shape)\n",
        "\n",
        "    batch_gen = self.data_loader.load_batch(batch_size)\n",
        "    for i, elem in enumerate(batch_gen):\n",
        "      break\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      steps_per_epoch = self.data_loader.n_batches\n",
        "      for batch_i in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "        imgs_I, imgs_S = next(batch_gen)\n",
        "        fake_S = self.g_IS.predict(imgs_I)\n",
        "        fake_I = self.g_SI.predict(imgs_S)\n",
        "\n",
        "        dI_loss_real = self.d_I.fit(imgs_I, valid)\n",
        "        dI_loss_fake = self.d_I.fit(fake_I, fake)\n",
        "\n",
        "        # dI_loss = 0.5 * np.add(dI_loss_real, dI_loss_fake)\n",
        "\n",
        "        dS_loss_real = self.d_S.fit(imgs_S, valid)\n",
        "        dS_loss_fake = self.d_S.fit(fake_S, fake)\n",
        "\n",
        "        # dS_loss = 0.5 * np.add(dS_loss_real, dS_loss_fake)\n",
        "\n",
        "        # d_loss = 0.5 * np.add(dI_loss, dS_loss)\n",
        "\n",
        "        g_loss = self.combined.fit([imgs_I, imgs_S], [fake, fake, imgs_I, imgs_S, imgs_I, imgs_S])\n",
        "\n",
        "        if batch_i % sample_interval == 0:\n",
        "          self.sample_images(epoch, batch_i)\n",
        "\n",
        "          # save weights (cannot save model because eager behavior disabled)\n",
        "          self.d_I.save_weights('d_I.weights')\n",
        "          shutil.copy('d_I.weights', os.path.join(project_path, 'd_I.weights'))\n",
        "\n",
        "          self.d_S.save_weights('d_S.weights')\n",
        "          shutil.copy('d_S.weights', os.path.join(project_path, 'd_S.weights'))\n",
        "\n",
        "          self.g_IS.save_weights('g_IS.weights')\n",
        "          shutil.copy('g_IS.weights', os.path.join(project_path, 'g_IS.weights'))\n",
        "\n",
        "          self.g_SI.save_weights('g_SI.weights')\n",
        "          shutil.copy('g_SI.weights', os.path.join(project_path, 'g_SI.weights'))\n",
        "  \n",
        "  def sample_images(self, epoch, batch_i):\n",
        "    format_size = 20\n",
        "    print('=' * format_size)\n",
        "\n",
        "    r, c = 2, 3\n",
        "\n",
        "    imgs_I = self.data_loader.load_data(domain=\"I\", batch_size=1, is_testing=True)\n",
        "    imgs_S = self.data_loader.load_data(domain=\"S\", batch_size=1, is_testing=True)\n",
        "    \n",
        "    # Translate images to the other domain\n",
        "    fake_S = self.g_IS.predict(imgs_I)\n",
        "    fake_I = self.g_SI.predict(imgs_S)\n",
        "    # Translate back to original domain\n",
        "    reconstr_I = self.g_SI.predict(fake_S)\n",
        "    reconstr_S = self.g_IS.predict(fake_I)\n",
        "\n",
        "    gen_imgs = np.concatenate([imgs_I, fake_S, reconstr_I, imgs_S, fake_I, reconstr_S])\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    titles = ['Original', 'Translated', 'Reconstructed']\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].set_title(titles[j])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    plt.show()\n",
        "\n",
        "    print(f'd_I on real imgs I: {self.d_I.predict(imgs_I)[0]}')\n",
        "    print(f'd_I on fake imgs I: {self.d_I.predict(fake_I)[0]}')\n",
        "    print(f'd_S on real imgs S: {self.d_S.predict(real_S)[0]}')\n",
        "    print(f'd_S on fake imgs S: {self.d_S.predict(fake_S)[0]}')\n",
        "\n",
        "    print('=' * format_size)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6YDDpkJNmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "e7b2380b-1207-46ed-bd9d-dc7b43205b03"
      },
      "source": [
        "gan = SegGAN()\n",
        "gan.train(epochs=10000, batch_size=5, sample_interval=100)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "\n",
            "\n",
            "Train 0 / 10000:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2753 - accuracy: 0.0016\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.2602 - accuracy: 0.0000e+00\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 58ms/step - loss: 0.2564 - accuracy: 0.0000e+00\n",
            "Epoch 1/1\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.2864 - accuracy: 0.0000e+00\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-1514fcada432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-8b01f5d59a26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# d_loss = 0.5 * np.add(dI_loss, dS_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_S\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_S\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msample_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3632\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdLAaAWqJqjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d41c86d1-f5b6-4c51-e331-0b395c2d74fe"
      },
      "source": [
        "img = gan.data_loader.imread(os.path.join(project_path, 'face_segment/trainI/image11700.jpg_RESAMPLED_b60438dccb6eb944_scaled.png'))\n",
        "img = np.resize(img, (128, 128, 3))\n",
        "img = np.expand_dims(img, axis=0)\n",
        "\n",
        "gan.d_I.predict(img)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[1.05982639e-01, 4.05439595e-03, 1.49216728e-09, ...,\n",
              "          2.19668448e-03, 9.53430772e-01, 9.99999881e-01],\n",
              "         [2.90256053e-01, 1.17753958e-02, 4.82435389e-08, ...,\n",
              "          1.29869087e-02, 8.93832982e-01, 9.99996305e-01],\n",
              "         [3.47467929e-01, 2.27841344e-02, 3.04162370e-07, ...,\n",
              "          2.37162411e-02, 8.40553522e-01, 9.99984384e-01],\n",
              "         ...,\n",
              "         [4.77579743e-01, 2.75121629e-01, 8.03640578e-04, ...,\n",
              "          4.05633897e-02, 5.21852314e-01, 9.89252150e-01],\n",
              "         [4.14973497e-01, 2.93227673e-01, 4.36048111e-04, ...,\n",
              "          4.02707681e-02, 5.00023484e-01, 9.92403388e-01],\n",
              "         [4.55743879e-01, 2.03325927e-01, 2.70809487e-05, ...,\n",
              "          1.72029305e-02, 5.46780288e-01, 9.98781025e-01]],\n",
              "\n",
              "        [[4.97036844e-01, 7.23325983e-02, 9.42923293e-07, ...,\n",
              "          6.60276087e-03, 7.10733891e-01, 9.99889612e-01],\n",
              "         [4.84209597e-01, 1.56586431e-02, 7.93060551e-09, ...,\n",
              "          1.88054994e-03, 8.61533403e-01, 9.99996662e-01],\n",
              "         [5.24049938e-01, 1.40706748e-01, 1.08299188e-04, ...,\n",
              "          2.62795649e-02, 6.56142473e-01, 9.97423649e-01],\n",
              "         ...,\n",
              "         [4.04068708e-01, 1.50675476e-01, 2.60025718e-05, ...,\n",
              "          4.14543413e-02, 6.29016101e-01, 9.99234915e-01],\n",
              "         [4.17337954e-01, 1.41708627e-01, 2.28985537e-05, ...,\n",
              "          3.87008004e-02, 6.37595177e-01, 9.99252975e-01],\n",
              "         [4.65979218e-01, 9.16876569e-02, 5.57669500e-06, ...,\n",
              "          2.59771254e-02, 6.98259652e-01, 9.99691606e-01]],\n",
              "\n",
              "        [[1.27123490e-01, 5.99845871e-03, 1.21008288e-08, ...,\n",
              "          7.01012230e-03, 9.27778006e-01, 9.99999285e-01],\n",
              "         [2.89775908e-01, 1.15453601e-02, 5.31366666e-08, ...,\n",
              "          1.52533045e-02, 8.92263293e-01, 9.99996305e-01],\n",
              "         [3.37973744e-01, 2.87735760e-02, 7.65205812e-07, ...,\n",
              "          3.19462605e-02, 8.20342124e-01, 9.99973416e-01],\n",
              "         ...,\n",
              "         [5.01835406e-01, 2.42096454e-01, 7.96151289e-04, ...,\n",
              "          4.53364477e-02, 5.61225474e-01, 9.89964306e-01],\n",
              "         [4.08146054e-01, 3.69490951e-01, 4.22614394e-03, ...,\n",
              "          9.33409482e-02, 4.83626038e-01, 9.68079686e-01],\n",
              "         [3.94584209e-01, 3.49711806e-01, 2.08509993e-03, ...,\n",
              "          8.64663944e-02, 5.01812100e-01, 9.81126130e-01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[4.53391463e-01, 1.44324070e-02, 3.29020255e-10, ...,\n",
              "          7.97253506e-06, 9.22784448e-01, 9.99999166e-01],\n",
              "         [4.65631008e-01, 1.66767649e-02, 1.26503397e-09, ...,\n",
              "          1.34116717e-05, 8.63299847e-01, 9.99998093e-01],\n",
              "         [4.54332292e-01, 1.73676237e-02, 6.77392642e-10, ...,\n",
              "          1.17560085e-05, 9.30887520e-01, 9.99998689e-01],\n",
              "         ...,\n",
              "         [3.93477350e-01, 2.17843265e-03, 2.46823438e-12, ...,\n",
              "          4.35688999e-05, 9.70277846e-01, 1.00000000e+00],\n",
              "         [4.93960559e-01, 6.70270715e-03, 1.87156776e-10, ...,\n",
              "          2.25630953e-04, 9.26914930e-01, 9.99999642e-01],\n",
              "         [5.37962735e-01, 3.80982645e-02, 1.43813224e-07, ...,\n",
              "          2.11935793e-03, 7.81184971e-01, 9.99966741e-01]],\n",
              "\n",
              "        [[4.03017312e-01, 1.03923837e-02, 7.79845521e-09, ...,\n",
              "          2.90226890e-03, 9.02813375e-01, 9.99997973e-01],\n",
              "         [3.65794212e-01, 1.28927166e-02, 1.39192577e-08, ...,\n",
              "          5.71867405e-03, 8.90512884e-01, 9.99997377e-01],\n",
              "         [3.42643291e-01, 1.04373265e-02, 9.10024056e-09, ...,\n",
              "          5.86033845e-03, 9.00823593e-01, 9.99998331e-01],\n",
              "         ...,\n",
              "         [4.86265063e-01, 8.29836912e-03, 1.47914528e-11, ...,\n",
              "          4.70105761e-06, 9.73940730e-01, 9.99999881e-01],\n",
              "         [4.95946527e-01, 6.66195992e-03, 4.62507159e-12, ...,\n",
              "          3.46492243e-06, 9.81065810e-01, 1.00000000e+00],\n",
              "         [4.95161474e-01, 6.93461765e-03, 6.74303278e-12, ...,\n",
              "          3.64212474e-06, 9.76078272e-01, 9.99999881e-01]],\n",
              "\n",
              "        [[4.82653469e-01, 9.39879101e-03, 3.97863825e-11, ...,\n",
              "          5.52038409e-06, 9.55861151e-01, 9.99999762e-01],\n",
              "         [4.77666020e-01, 1.17928637e-02, 1.62931516e-10, ...,\n",
              "          8.37345669e-06, 9.28302467e-01, 9.99999523e-01],\n",
              "         [4.74118501e-01, 1.23517783e-02, 1.38357062e-10, ...,\n",
              "          9.16575573e-06, 9.50705409e-01, 9.99999523e-01],\n",
              "         ...,\n",
              "         [3.28497767e-01, 1.24863267e-03, 6.20790745e-13, ...,\n",
              "          4.69142105e-05, 9.72586572e-01, 1.00000000e+00],\n",
              "         [4.18219954e-01, 2.56095966e-03, 9.35384929e-12, ...,\n",
              "          1.23584294e-04, 9.54630136e-01, 1.00000000e+00],\n",
              "         [5.26025653e-01, 1.61514115e-02, 5.21922372e-09, ...,\n",
              "          1.08238426e-03, 8.51140916e-01, 9.99996781e-01]]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqhTN97V1TE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4bf65e65-9d4a-4090-9912-52a28740c669"
      },
      "source": [
        "project_path"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/hairy_gan/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3aneO1Z1YQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}