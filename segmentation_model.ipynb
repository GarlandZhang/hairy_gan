{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I61kYxdeY_e-y7_7sNtBKbCVJMvoqLJ0",
      "authorship_tag": "ABX9TyOLyJlERgxxaSfWYw0mva7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/segmentation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmArMn_FGXpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "project_path = '/content/drive/My Drive/hairy_gan/'\n",
        "dataset_path = os.path.join(project_path, 'face_segment')\n",
        "src_img_target_path = os.path.join(dataset_path, 'trainI')\n",
        "label_img_target_path = os.path.join(dataset_path, 'trainS')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkczVm_WBHyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move images and corresponding segmentation masks into their own folders\n",
        "# tree = ET.parse(os.path.join(dataset_path, 'sampleset.xml'))\n",
        "# root = tree.getroot()\n",
        "# children = root.getchildren()\n",
        "# for i in range(len(children) // 2):\n",
        "#   src_img_path = children[2 * i].get('name').replace('\\\\', '/')\n",
        "#   label_img_path = children[2 * i + 1].get('name').replace('\\\\', '/')\n",
        "  \n",
        "#   shutil.copy(os.path.join(dataset_path, src_img_path), os.path.join(src_img_target_path, os.path.basename(src_img_path)))\n",
        "#   shutil.copy(os.path.join(dataset_path, label_img_path), os.path.join(label_img_target_path, os.path.basename(label_img_path)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2WmmlCzDYKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ab4aa650-000f-43c0-8939-efc64ee39700"
      },
      "source": [
        "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "!pip install pillow\n",
        "!pip install scipy==1.1.0        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'keras-contrib' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QsonbJdyaye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ae4f37d4-3640-4e91-aa44-3da6e8cd9a3c"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import shutil\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import PIL\n",
        "from glob import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqcAQNxPGMQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_path, img_res=(128, 128)):\n",
        "      self.dataset_path = dataset_path\n",
        "      self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "      # data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "      data_type = 'train' + domain # always grab from training since we have no test\n",
        "      path = glob(os.path.join(self.dataset_path, data_type, '*'))\n",
        "\n",
        "      batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "      imgs = []\n",
        "      for img_path in batch_images:\n",
        "          img = self.imread(img_path)\n",
        "          if not is_testing:\n",
        "              img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "              if np.random.random() > 0.5:\n",
        "                  img = np.fliplr(img)\n",
        "          else:\n",
        "              img = scipy.misc.imresize(img, self.img_res)\n",
        "          imgs.append(img)\n",
        "\n",
        "      imgs = np.array(imgs)\n",
        "\n",
        "      if domain == 'I':\n",
        "        imgs = imgs /127.5 - 1.\n",
        "\n",
        "      return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "      data_type = \"train\" if not is_testing else \"val\"\n",
        "      path_I = glob(os.path.join(self.dataset_path, data_type + 'I', '*'))\n",
        "      path_S = glob(os.path.join(self.dataset_path, data_type + 'S', '*'))\n",
        "\n",
        "      self.n_batches = int(min(len(path_I), len(path_S)) / batch_size)\n",
        "      total_samples = self.n_batches * batch_size\n",
        "\n",
        "      # Sample n_batches * batch_size from each path list so that model sees all\n",
        "      # samples from both domains\n",
        "      path_I = np.random.choice(path_I, total_samples, replace=False)\n",
        "      path_S = np.random.choice(path_S, total_samples, replace=False)\n",
        "\n",
        "      i = 0\n",
        "      while True:\n",
        "          batch_I = path_I[i*batch_size:(i+1)*batch_size]\n",
        "          batch_S = path_S[i*batch_size:(i+1)*batch_size]\n",
        "          imgs_I, imgs_S = [], []\n",
        "          for img_I, img_S in zip(batch_I, batch_S):\n",
        "              img_I = self.imread(img_I)\n",
        "              img_S = self.imread(img_S)\n",
        "\n",
        "              img_I = scipy.misc.imresize(img_I, self.img_res)\n",
        "              img_S = scipy.misc.imresize(img_S, self.img_res)\n",
        "\n",
        "              if not is_testing and np.random.random() > 0.5:\n",
        "                      img_I = np.fliplr(img_I)\n",
        "                      img_S = np.fliplr(img_S)\n",
        "\n",
        "              imgs_I.append(img_I)\n",
        "              imgs_S.append(img_S)\n",
        "\n",
        "          imgs_I = np.array(imgs_I)/127.5 - 1.\n",
        "          imgs_S = np.array(imgs_S)\n",
        "\n",
        "          i += 1\n",
        "\n",
        "          if i == self.n_batches - 1:\n",
        "            path_I = np.random.choice(path_I, total_samples, replace=False)\n",
        "            path_S = np.random.choice(path_S, total_samples, replace=False)\n",
        "            i = 0\n",
        "          yield imgs_I, imgs_S\n",
        "\n",
        "    def imread(self, path):\n",
        "      return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7awGdclIMOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SegGAN():\n",
        "  def __init__(self):\n",
        "    self.img_rows = 128\n",
        "    self.img_cols = 128\n",
        "    self.img_channels = 3\n",
        "\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "\n",
        "    self.data_loader = DataLoader(dataset_path, img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "    # patch = int(self.img_rows / 2**4)\n",
        "    # self.disc_patch = (patch, patch, 1) # output shape of discriminator\n",
        "\n",
        "    self.gf = 32 # num filters in first layer of gen\n",
        "    self.df = 64 # num filters in first layer of disc\n",
        "\n",
        "    optimizer = Adam(0.0002, 0.5, 0.999)\n",
        "\n",
        "    self.d_I = self.build_discriminator()\n",
        "    self.d_S = self.build_discriminator()\n",
        "\n",
        "    self.d_I.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "    self.d_S.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    self.d_output_shape = tuple(self.d_I.layers[-1].output.get_shape().as_list()[1:])\n",
        "\n",
        "    self.g_IS = self.build_generator(categorical=True)\n",
        "    self.g_SI = self.build_generator()\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'd_I.weights')):\n",
        "      self.d_I.load_weights(os.path.join(project_path, 'd_I.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'd_S.weights')):\n",
        "      self.d_S.load_weights(os.path.join(project_path, 'd_S.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'g_IS.weights')):\n",
        "      self.g_IS.load_weights(os.path.join(project_path, 'g_IS.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'g_SI.weights')):\n",
        "      self.g_SI.load_weights(os.path.join(project_path, 'g_SI.weights'))\n",
        "\n",
        "    img_I = Input(shape=self.img_shape)\n",
        "    img_S = Input(shape=self.img_shape)\n",
        "\n",
        "    fake_S = self.g_IS(img_I)\n",
        "    fake_I = self.g_SI(img_S)\n",
        "\n",
        "    reconstr_I = self.g_SI(fake_S)\n",
        "    reconstr_S = self.g_IS(fake_I)\n",
        "\n",
        "    valid_I = self.d_I(fake_I)\n",
        "    valid_S = self.d_S(fake_S)\n",
        "\n",
        "    self.combined = Model(inputs=[img_I, img_S], outputs=[valid_I, valid_S, fake_S, fake_I, reconstr_I, reconstr_S])\n",
        "    self.combined.compile(loss=['mse', 'mse', 'sparse_categorical_crossentropy', 'mse', 'mae', 'sparse_categorical_crossentropy'], loss_weights=[1, 1, 1, 1, 1, 0.05], optimizer=optimizer)\n",
        "\n",
        "  def build_generator(self, categorical=False):\n",
        "    '''U-net generator'''\n",
        "\n",
        "    def conv2d(layer_input, filters, f_size=4):\n",
        "      '''Layers used during downsampling'''\n",
        "      d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      d = InstanceNormalization()(d)\n",
        "      return d\n",
        "\n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "      '''Layers used during upsampling'''\n",
        "      u = UpSampling2D(size=2)(layer_input)\n",
        "      u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "\n",
        "      if dropout_rate:\n",
        "        u = Dropout(dropout_rate)(u)\n",
        "      u = InstanceNormalization()(u)\n",
        "      u = Concatenate()([u, skip_input])\n",
        "      return u\n",
        "\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = conv2d(d0, self.gf)\n",
        "    d2 = conv2d(d1, self.gf * 2)\n",
        "    d3 = conv2d(d2, self.gf * 4)\n",
        "    d4 = conv2d(d3, self.gf * 8)\n",
        "\n",
        "    u1 = deconv2d(d4, d3, self.gf * 4)\n",
        "    u2 = deconv2d(u1, d2, self.gf * 2)\n",
        "    u3 = deconv2d(u2, d1, self.gf)\n",
        "\n",
        "    u4 = UpSampling2D(size=2)(u3)\n",
        "    output_img = Conv2D(self.img_channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "    return Model(d0, output_img)\n",
        "\n",
        "  def build_discriminator(self):\n",
        "  \n",
        "    input = Input(shape=self.img_shape)\n",
        "    d1 = Conv2D(self.df, kernel_size=1, strides=1, padding='same')(input)\n",
        "    d2 = LeakyReLU(alpha=0.2)(d1)\n",
        "    d3 = Conv2D(self.df * 2, kernel_size=1, strides=1, padding='same')(d2)\n",
        "    d4 = BatchNormalization()(d3)\n",
        "    d5 = LeakyReLU(alpha=0.2)(d4)\n",
        "    d6 = Conv2D(self.df * 2, kernel_size=1, strides=1, padding='same', activation='sigmoid')(d5)\n",
        "\n",
        "    return Model(input, d6)\n",
        "\n",
        "  def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    valid = np.ones((batch_size, ) + self.d_output_shape)\n",
        "\n",
        "    fake = np.zeros((batch_size, ) + self.d_output_shape)\n",
        "\n",
        "    batch_gen = self.data_loader.load_batch(batch_size)\n",
        "    for i, elem in enumerate(batch_gen):\n",
        "      break\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      steps_per_epoch = self.data_loader.n_batches\n",
        "      for batch_i in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "        imgs_I, imgs_S = next(batch_gen)\n",
        "        fake_S = self.g_IS.predict(imgs_I)\n",
        "        fake_I = self.g_SI.predict(imgs_S)\n",
        "\n",
        "        dI_loss_real = self.d_I.fit(imgs_I, valid)\n",
        "        dI_loss_fake = self.d_I.fit(fake_I, fake)\n",
        "\n",
        "        # dI_loss = 0.5 * np.add(dI_loss_real, dI_loss_fake)\n",
        "\n",
        "        dS_loss_real = self.d_S.fit(imgs_S, valid)\n",
        "        dS_loss_fake = self.d_S.fit(fake_S, fake)\n",
        "\n",
        "        # dS_loss = 0.5 * np.add(dS_loss_real, dS_loss_fake)\n",
        "\n",
        "        # d_loss = 0.5 * np.add(dI_loss, dS_loss)\n",
        "\n",
        "        g_loss = self.combined.fit([imgs_I, imgs_S], [fake, fake, imgs_I, imgs_S, imgs_I, imgs_S])\n",
        "\n",
        "        if batch_i % sample_interval == 0:\n",
        "          self.sample_images(epoch, batch_i)\n",
        "\n",
        "          # save weights (cannot save model because eager behavior disabled)\n",
        "          self.d_I.save_weights('d_I.weights')\n",
        "          shutil.copy('d_I.weights', os.path.join(project_path, 'd_I.weights'))\n",
        "\n",
        "          self.d_S.save_weights('d_S.weights')\n",
        "          shutil.copy('d_S.weights', os.path.join(project_path, 'd_S.weights'))\n",
        "\n",
        "          self.g_IS.save_weights('g_IS.weights')\n",
        "          shutil.copy('g_IS.weights', os.path.join(project_path, 'g_IS.weights'))\n",
        "\n",
        "          self.g_SI.save_weights('g_SI.weights')\n",
        "          shutil.copy('g_SI.weights', os.path.join(project_path, 'g_SI.weights'))\n",
        "  \n",
        "  def sample_images(self, epoch, batch_i):\n",
        "    format_size = 20\n",
        "    print('=' * format_size)\n",
        "\n",
        "    r, c = 2, 3\n",
        "\n",
        "    imgs_I = self.data_loader.load_data(domain=\"I\", batch_size=1, is_testing=True)\n",
        "    imgs_S = self.data_loader.load_data(domain=\"S\", batch_size=1, is_testing=True)\n",
        "    \n",
        "    # Translate images to the other domain\n",
        "    fake_S = self.g_IS.predict(imgs_I)\n",
        "    fake_I = self.g_SI.predict(imgs_S)\n",
        "    # Translate back to original domain\n",
        "    reconstr_I = self.g_SI.predict(fake_S)\n",
        "    reconstr_S = self.g_IS.predict(fake_I)\n",
        "\n",
        "    gen_imgs = np.concatenate([imgs_I, fake_S, reconstr_I, imgs_S, fake_I, reconstr_S])\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    titles = ['Original', 'Translated', 'Reconstructed']\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].set_title(titles[j])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    plt.show()\n",
        "\n",
        "    print(f'd_I on real imgs I: {self.d_I.predict(imgs_I)[0]}')\n",
        "    print(f'd_I on fake imgs I: {self.d_I.predict(fake_I)[0]}')\n",
        "    print(f'd_S on real imgs S: {self.d_S.predict(imgs_S)[0]}')\n",
        "    print(f'd_S on fake imgs S: {self.d_S.predict(fake_S)[0]}')\n",
        "\n",
        "    print('=' * format_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6YDDpkJNmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f95a871d-c465-4bdd-f2dd-2d1b4eabb9fe"
      },
      "source": [
        "gan = SegGAN()\n",
        "# gan.train(epochs=10000, batch_size=5, sample_interval=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgpjCF0Fe4qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import glob\n",
        "# import os\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# project_path = '/content/drive/My Drive/hairy_gan'\n",
        "# files = glob.glob(os.path.join(project_path, 'face_segment', 'trainS', '*'))\n",
        "# target_folder = os.path.join(project_path, 'face_segment', 'trainS_2')\n",
        "\n",
        "# for f in files:\n",
        "#   img = cv2.imread(f)\n",
        "#   img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#   target_img = np.where(img, 1, 0)[:, :, 0]\n",
        "#   cv2.imwrite(os.path.join(target_folder, os.path.basename(f)), target_img)"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}