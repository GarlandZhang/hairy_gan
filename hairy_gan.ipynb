{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hairy_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PFgBE9Rkd8fOWpAi2lsgE1RrQYJKfdKa",
      "authorship_tag": "ABX9TyOhKV+jIA2MTg0+c/7YgG9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/hairy_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb_cPA4QnnNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "if not os.path.exists('kaggle.json'):\n",
        "  !pip install -q kaggle\n",
        "  files.upload()\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !kaggle datasets download -d jessicali9530/celeba-dataset --force\n",
        "  !unzip celeba-dataset.zip\n",
        "  !mv img_align_celeba celeba-dataset\n",
        "  !mv list_eval_partition.csv celeba-dataset/list_eval_partition.csv\n",
        "  !mv list_landmarks_align_celeba.csv celeba-dataset/list_landmarks_align_celeba.csv\n",
        "  !mv list_attr_celeba.csv celeba-dataset/list_attr_celeba.csv\n",
        "  !mv list_bbox_celeba.csv celeba-dataset/list_bbox_celeba.csv\n",
        "\n",
        "  !mkdir celeba-dataset/trainA\n",
        "  !mkdir celeba-dataset/trainB\n",
        "  !mkdir celeba-dataset/validationA\n",
        "  !mkdir celeba-dataset/validationB\n",
        "  !mkdir celeba-dataset/testA\n",
        "  !mkdir celeba-dataset/testB\n",
        "\n",
        "  complete_df = pd.read_csv('celeba-dataset/list_attr_celeba.csv')\n",
        "  partitions_df = pd.read_csv('celeba-dataset/list_eval_partition.csv') # 0 => train, 1 => validation, 2 => test\n",
        "  for i, set_name in enumerate(['train', 'validation', 'test']):\n",
        "    set_ids_df = partitions_df.loc[partitions_df['partition'] == i]['image_id']\n",
        "    complete_df[complete_df['image_id'].isin(set_ids_df.tolist())]\n",
        "    black_hair_ids = complete_df.loc[complete_df['Black_Hair'] == 1]['image_id'].tolist() # filters out black hair candidates\n",
        "    blond_hair_ids = complete_df.loc[complete_df['Blond_Hair'] == 1]['image_id'].tolist() # filters out blond hair candidates\n",
        "    for id in black_hair_ids:\n",
        "      shutil.copy(os.path.join('celeba-dataset/img_align_celeba', id), os.path.join('celeba-dataset', f'{set_name}A', id))\n",
        "\n",
        "    for id in blond_hair_ids:\n",
        "      shutil.copy(os.path.join('celeba-dataset/img_align_celeba', id), os.path.join('celeba-dataset', f'{set_name}B', id))  \n",
        "\n",
        "  !git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "  !pip install scipy==1.1.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_tQXJCU83oV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b047636a-b104-46b4-ceea-d398c2f62b38"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.backend import set_session\n",
        "from tensorflow.python.keras.models import load_model\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpW0Uqm7v6y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "\n",
        "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
        "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path_A = glob('%s/%sA/*' % (self.dataset_name, data_type))\n",
        "        path_B = glob('%s/%sB/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        # Sample n_batches * batch_size from each path list so that model sees all\n",
        "        # samples from both domains\n",
        "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
        "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
        "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
        "            imgs_A, imgs_B = [], []\n",
        "            for img_A, img_B in zip(batch_A, batch_B):\n",
        "                img_A = self.imread(img_A)\n",
        "                img_B = self.imread(img_B)\n",
        "\n",
        "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
        "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img_A = np.fliplr(img_A)\n",
        "                        img_B = np.fliplr(img_B)\n",
        "\n",
        "                imgs_A.append(img_A)\n",
        "                imgs_B.append(img_B)\n",
        "\n",
        "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
        "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
        "\n",
        "            yield imgs_A, imgs_B\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbgXa0M9aWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HairyGan(): # based on CycleGan\n",
        "  def __init__(self):\n",
        "    \n",
        "    # self.sess = tf.compat.v1.Session()\n",
        "    # set_session(self.sess)\n",
        "    \n",
        "    # self.graph = tf.compat.v1.get_default_graph()\n",
        "    \n",
        "    # self.graph = tf.Graph()\n",
        "    # self.graph2 = tf.Graph()\n",
        "\n",
        "    self.img_rows = 128\n",
        "    self.img_cols = 128\n",
        "    self.img_channels = 3\n",
        "\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "    \n",
        "    patch = int(self.img_rows / 2**4)\n",
        "    self.disc_out = (patch, patch, 1) # output shape of discriminator\n",
        "\n",
        "    self.data_loader = DataLoader(dataset_name='celeba-dataset', img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "    self.optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "    # with self.graph.as_default():\n",
        "    # discriminators\n",
        "    self.d_A = self.build_discriminator()\n",
        "    self.d_B = self.build_discriminator()\n",
        "\n",
        "    self.d_A.compile(loss='mse', optimizer=self.optimizer, metrics=['accuracy'])\n",
        "    self.d_B.compile(loss='mse', optimizer=self.optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # generators\n",
        "    self.g_AB = self.build_generator()\n",
        "    self.g_BA = self.build_generator()\n",
        "\n",
        "    self.combined_model = self.build_combined()\n",
        "\n",
        "  def build_combined(self):\n",
        "    lambda_cycle = 10.\n",
        "    lambda_id = 0.9 * lambda_cycle\n",
        "\n",
        "    img_A = Input(shape=self.img_shape) # image from domain A\n",
        "    img_B = Input(shape=self.img_shape) # image from domain B\n",
        "\n",
        "    fake_A = self.g_BA(img_B)\n",
        "    fake_B = self.g_AB(img_A)\n",
        "    \n",
        "    reconstr_A = self.g_BA(fake_B)\n",
        "    reconstr_B = self.g_AB(fake_A)\n",
        "\n",
        "    id_A = self.g_BA(img_A)\n",
        "    id_B = self.g_AB(img_B)\n",
        "\n",
        "    valid_A = self.d_A(fake_A)\n",
        "    valid_B = self.d_B(fake_B)\n",
        "\n",
        "    combined = Model(\n",
        "        inputs=[img_A, img_B],\n",
        "        outputs=[valid_A, valid_B, reconstr_A, reconstr_B, id_A, id_B]\n",
        "    )\n",
        "\n",
        "    combined.compile(\n",
        "        loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'],\n",
        "        loss_weights=[1, 1, lambda_cycle, lambda_cycle, lambda_id, lambda_id],\n",
        "        optimizer=self.optimizer\n",
        "    )\n",
        "\n",
        "    return combined\n",
        "\n",
        "  def build_discriminator(self):\n",
        "\n",
        "    def d_conv_block(layer_input, num_filt, f_size=4, normalization=True):\n",
        "      d = Conv2D(num_filt, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      if normalization:\n",
        "        d = InstanceNormalization()(d)\n",
        "\n",
        "      return d\n",
        "\n",
        "    num_filt = 64\n",
        "\n",
        "    img = Input(shape=self.img_shape)\n",
        "    d1 = d_conv_block(img, num_filt, normalization=False)\n",
        "    d2 = d_conv_block(d1, num_filt * 2)\n",
        "    d3 = d_conv_block(d2, num_filt * 4)\n",
        "    d4 = d_conv_block(d3, num_filt * 8)\n",
        "    output = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "    return Model(img, output)\n",
        "\n",
        "  def build_generator(self):\n",
        "    def g_conv_block(layer_input, num_filt, f_size=4):\n",
        "      d = Conv2D(num_filt, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "      d = LeakyReLU(alpha=0.2)(d)\n",
        "      d = InstanceNormalization()(d)\n",
        "      return d\n",
        "\n",
        "    def g_deconv_block(layer_input, skip_input, num_filt, f_size=4, dropout_rate=0):\n",
        "      u = UpSampling2D(size=2)(layer_input)\n",
        "      u = Conv2D(num_filt, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "      \n",
        "      if dropout_rate:\n",
        "        u = Dropout(dropout_rate)(u)\n",
        "      \n",
        "      u = InstanceNormalization()(u)\n",
        "      u = Concatenate()([u, skip_input])\n",
        "\n",
        "      return u\n",
        "\n",
        "    num_filt = 32\n",
        "\n",
        "    d0 = Input(shape=self.img_shape)\n",
        "    d1 = g_conv_block(d0, num_filt)\n",
        "    d2 = g_conv_block(d1, num_filt * 2)\n",
        "    d3 = g_conv_block(d2, num_filt * 4)\n",
        "    d4 = g_conv_block(d3, num_filt * 8)\n",
        "\n",
        "    u1 = g_deconv_block(d4, d3, num_filt * 4)\n",
        "    u2 = g_deconv_block(u1, d2, num_filt * 2)\n",
        "    u3 = g_deconv_block(u2, d1, num_filt)\n",
        "\n",
        "    u4 = UpSampling2D(size=2)(u3)\n",
        "    output = Conv2D(self.img_channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "    return Model(d0, output)\n",
        "\n",
        "  def train_fake(self, epochs, batch_size=1, sample_interval=50):\n",
        "\n",
        "    # with self.graph.as_default():\n",
        "    # discriminators\n",
        "    self.d_A = self.build_discriminator()\n",
        "    self.d_B = self.build_discriminator()\n",
        "\n",
        "    self.d_A.compile(loss='mse', optimizer=self.optimizer, metrics=['accuracy'])\n",
        "    self.d_B.compile(loss='mse', optimizer=self.optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # generators\n",
        "    self.g_AB = self.build_generator()\n",
        "    self.g_BA = self.build_generator()\n",
        "\n",
        "    self.combined_model = self.build_combined()\n",
        "\n",
        "\n",
        "    d_out_shape = (batch_size, ) + self.disc_out\n",
        "\n",
        "    valid = np.ones(d_out_shape)\n",
        "    fake = np.zeros(d_out_shape)\n",
        "\n",
        "    # with self.graph.as_default():\n",
        "      # set_session(self.sess)\n",
        "    for epoch in range(epochs):\n",
        "      batch_gen = self.data_loader.load_batch(batch_size)\n",
        "      for i, elem in enumerate(batch_gen):\n",
        "        break# hack to run function once\n",
        "\n",
        "      steps_per_epoch = self.data_loader.n_batches - 2\n",
        "      for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "        batch_i = step\n",
        "\n",
        "        imgs_A, imgs_B = next(batch_gen)\n",
        "\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "\n",
        "        real_A_history = self.d_A.fit(imgs_A, valid, batch_size=batch_size)\n",
        "        fake_A_history = self.d_A.fit(fake_A, fake, batch_size=batch_size)\n",
        "        # d_A_loss = 0.5 * np.add(real_A_loss, fake_A_loss)\n",
        "\n",
        "        real_B_history = self.d_B.fit(imgs_B, valid, batch_size=batch_size)\n",
        "        fake_B_history = self.d_B.fit(fake_B, fake, batch_size=batch_size)\n",
        "        # d_B_loss = 0.5 * np.add(real_B_loss, fake_B_loss)\n",
        "\n",
        "        # d_loss = 0.5 * np.add(d_A_loss, d_B_loss)\n",
        "\n",
        "        g_history = self.combined_model.fit(\n",
        "            [imgs_A, imgs_B],\n",
        "            [fake, fake, imgs_A, imgs_B, imgs_A, imgs_B],\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        if (batch_i + 1) % sample_interval == 0:\n",
        "          self.sample_images(epoch, batch_i)\n",
        "          project_path = '/content/drive/My Drive/hairy_gan'\n",
        "          self.d_A.save('d_A.hdf5')\n",
        "          shutil.copy('d_A.hdf5', os.path.join(project_path, 'd_A.hdf5'))\n",
        "\n",
        "          self.d_B.save('d_B.hdf5')\n",
        "          shutil.copy('d_B.hdf5', os.path.join(project_path, 'd_B.hdf5'))\n",
        "\n",
        "\n",
        "          self.g_AB.save('g_AB.hdf5')\n",
        "          shutil.copy('g_AB.hdf5', os.path.join(project_path, 'g_AB.hdf5'))\n",
        "\n",
        "          self.g_BA.save('g_BA.hdf5')\n",
        "          shutil.copy('g_BA.hdf5', os.path.join(project_path, 'g_BA.hdf5'))\n",
        "\n",
        "  def train(self, epochs, batch_size=1, sample_interval=50):\n",
        "    d_out_shape = (batch_size, ) + gan.disc_out\n",
        "\n",
        "    valid = np.ones(d_out_shape)\n",
        "\n",
        "    # sess = tf.compat.v1.Session()\n",
        "\n",
        "    # with self.graph.as_default():\n",
        "    self.d_A = self.build_discriminator()\n",
        "    self.d_A.compile(loss='mse', optimizer=Adam(), metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    # with self.graph2.as_default():\n",
        "    self.d_B = self.build_discriminator()\n",
        "    self.d_B.compile(loss='mse', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "    # generators\n",
        "    self.g_AB = self.build_generator()\n",
        "    self.g_BA = self.build_generator()\n",
        "\n",
        "    self.combined_model = self.build_combined()\n",
        "\n",
        "\n",
        "    d_out_shape = (batch_size, ) + self.disc_out\n",
        "\n",
        "    valid = np.ones(d_out_shape)\n",
        "    fake = np.zeros(d_out_shape)\n",
        "\n",
        "    batch_gen = self.data_loader.load_batch(batch_size)\n",
        "    for epoch in range(epochs):\n",
        "      batch_gen = self.data_loader.load_batch(batch_size)\n",
        "      for i, elem in enumerate(batch_gen):\n",
        "        break# hack to run function once\n",
        "\n",
        "      steps_per_epoch = self.data_loader.n_batches - 2\n",
        "      for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "        batch_i = step\n",
        "\n",
        "        imgs_A, imgs_B = next(batch_gen)\n",
        "\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "\n",
        "        real_A_history = self.d_A.fit(imgs_A, valid, batch_size=batch_size)\n",
        "        fake_A_history = self.d_A.fit(fake_A, fake, batch_size=batch_size)\n",
        "        # d_A_loss = 0.5 * np.add(real_A_loss, fake_A_loss)\n",
        "\n",
        "        real_B_history = self.d_B.fit(imgs_B, valid, batch_size=batch_size)\n",
        "        fake_B_history = self.d_B.fit(fake_B, fake, batch_size=batch_size)\n",
        "        # d_B_loss = 0.5 * np.add(real_B_loss, fake_B_loss)\n",
        "\n",
        "        # d_loss = 0.5 * np.add(d_A_loss, d_B_loss)\n",
        "\n",
        "        g_history = self.combined_model.fit(\n",
        "            [imgs_A, imgs_B],\n",
        "            [fake, fake, imgs_A, imgs_B, imgs_A, imgs_B],\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        if (batch_i + 1) % sample_interval == 0:\n",
        "          self.sample_images(epoch, batch_i)\n",
        "          project_path = '/content/drive/My Drive/hairy_gan'\n",
        "          self.d_A.save('d_A.hdf5')\n",
        "          shutil.copy('d_A.hdf5', os.path.join(project_path, 'd_A.hdf5'))\n",
        "\n",
        "          self.d_B.save('d_B.hdf5')\n",
        "          shutil.copy('d_B.hdf5', os.path.join(project_path, 'd_B.hdf5'))\n",
        "\n",
        "\n",
        "          self.g_AB.save('g_AB.hdf5')\n",
        "          shutil.copy('g_AB.hdf5', os.path.join(project_path, 'g_AB.hdf5'))\n",
        "\n",
        "          self.g_BA.save('g_BA.hdf5')\n",
        "          shutil.copy('g_BA.hdf5', os.path.join(project_path, 'g_BA.hdf5'))\n",
        "\n",
        "  def sample_images(self, epoch, batch_i):\n",
        "    rows, cols = 2, 3\n",
        "\n",
        "    imgs_A = self.data_loader.load_data(domain='A', batch_size=1, is_testing=True)\n",
        "    imgs_B = self.data_loader.load_data(domain='B', batch_size=1, is_testing=True)\n",
        "\n",
        "    fake_A = self.g_BA.predict(imgs_B)\n",
        "    fake_B = self.g_AB.predict(imgs_A)\n",
        "\n",
        "    reconstr_A = self.g_BA.predict(fake_B)\n",
        "    reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "    gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    titles = ['Original', 'Translated', 'Reconstructed']\n",
        "    fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axes[i, j].imshow(gen_imgs[count])\n",
        "        axes[i, j].set_title(titles[j])\n",
        "        axes[i, j].axis('off')\n",
        "        count += 1\n",
        "\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCeFNls66FAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef3b0225-fc9d-475e-f1a5-f0b8d82232e1"
      },
      "source": [
        "gan = HairyGan()\n",
        "gan.train(epochs=100, batch_size=64, sample_interval=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   0%|          | 0/466 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 7ms/step - loss: 1.5522 - accuracy: 0.2908\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 186.2229 - accuracy: 0.0088\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 7ms/step - loss: 1.2294 - accuracy: 0.3521\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 107.4345 - accuracy: 0.0322\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 7s 104ms/step - loss: 490.3771 - model_275_loss: 279.3494 - model_276_loss: 187.6611 - model_278_loss: 0.5554 - model_277_loss: 0.6607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   0%|          | 1/466 [00:27<3:31:01, 27.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 46.0453 - accuracy: 0.0771\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 24.8009 - accuracy: 0.0710\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 36.9366 - accuracy: 0.0342\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 124.6796 - accuracy: 0.0017\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 50.7935 - model_275_loss: 20.2086 - model_276_loss: 6.9510 - model_278_loss: 0.4075 - model_277_loss: 0.6245\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   0%|          | 2/466 [00:29<2:33:15, 19.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 2.3305 - accuracy: 0.2644\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 4.5813 - accuracy: 0.1350\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 3.4686 - accuracy: 0.1919\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.6463 - accuracy: 0.2258\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 29.7386 - model_275_loss: 2.8351 - model_276_loss: 5.4707 - model_278_loss: 0.3745 - model_277_loss: 0.5973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   1%|          | 3/466 [00:32<1:52:56, 14.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 3.9021 - accuracy: 0.1226\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.7667 - accuracy: 0.2258\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 7.9226 - accuracy: 7.3242e-04\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 4.2496 - accuracy: 0.0566\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 24.0333 - model_275_loss: 1.8973 - model_276_loss: 2.5425 - model_278_loss: 0.3762 - model_277_loss: 0.5607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   1%|          | 4/466 [00:34<1:24:46, 11.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 3.1825 - accuracy: 0.1167\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.9718 - accuracy: 0.3069\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 4.4033 - accuracy: 0.0044\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.5735\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 19.9106 - model_275_loss: 1.5991 - model_276_loss: 0.7152 - model_278_loss: 0.3442 - model_277_loss: 0.5385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   1%|          | 5/466 [00:37<1:05:06,  8.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.3794 - accuracy: 0.3457\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.3302 - accuracy: 0.3384\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.5657\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 2.6449 - accuracy: 0.1250\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 21.6778 - model_275_loss: 1.4906 - model_276_loss: 3.7110 - model_278_loss: 0.3358 - model_277_loss: 0.5059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   1%|▏         | 6/466 [00:39<51:25,  6.71s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.6652 - accuracy: 0.3015\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.9279 - accuracy: 0.4519\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.7089 - accuracy: 0.4150\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 3.1459 - accuracy: 0.0469\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 20.2110 - model_275_loss: 0.8142 - model_276_loss: 2.3398 - model_278_loss: 0.3630 - model_277_loss: 0.5159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   2%|▏         | 7/466 [00:42<41:49,  5.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.2756 - accuracy: 0.3743\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.5930\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.7095\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.8551 - accuracy: 0.3723\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 16.5605 - model_275_loss: 0.4816 - model_276_loss: 0.4111 - model_278_loss: 0.3423 - model_277_loss: 0.4701\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   2%|▏         | 8/466 [00:45<35:10,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.2730 - accuracy: 0.3186\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3837 - accuracy: 0.6003\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.9525 - accuracy: 0.1775\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.6438\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 16.3060 - model_275_loss: 0.4493 - model_276_loss: 0.3239 - model_278_loss: 0.3365 - model_277_loss: 0.4759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   2%|▏         | 9/466 [00:47<30:28,  4.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.5119 - accuracy: 0.2190\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.5957\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.7282 - accuracy: 0.0549\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.6604\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 15.6688 - model_275_loss: 0.3813 - model_276_loss: 0.2155 - model_278_loss: 0.3222 - model_277_loss: 0.4703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   2%|▏         | 10/466 [00:50<27:12,  3.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.4845 - accuracy: 0.2478\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.7280\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.9797 - accuracy: 0.1152\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.6443\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 15.3890 - model_275_loss: 0.2801 - model_276_loss: 0.5681 - model_278_loss: 0.3124 - model_277_loss: 0.4589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   2%|▏         | 11/466 [00:53<25:03,  3.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.2755 - accuracy: 0.3545\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.6987\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.5027\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.7927 - accuracy: 0.2754\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 15.7617 - model_275_loss: 0.3719 - model_276_loss: 0.9731 - model_278_loss: 0.3139 - model_277_loss: 0.4550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   3%|▎         | 12/466 [00:55<23:24,  3.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.7988 - accuracy: 0.4651\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.5911\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.6572\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.3025\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 14.6777 - model_275_loss: 0.5609 - model_276_loss: 0.5875 - model_278_loss: 0.2905 - model_277_loss: 0.4285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   3%|▎         | 13/466 [00:58<22:10,  2.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.7912 - accuracy: 0.4651\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.6006\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.3767\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.6357\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 14.0507 - model_275_loss: 0.4083 - model_276_loss: 0.2437 - model_278_loss: 0.2975 - model_277_loss: 0.4185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   3%|▎         | 14/466 [01:00<21:18,  2.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6302 - accuracy: 0.4961\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.6631\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6794 - accuracy: 0.1538\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.8186\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 12.9498 - model_275_loss: 0.2946 - model_276_loss: 0.1890 - model_278_loss: 0.2664 - model_277_loss: 0.3965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   3%|▎         | 15/466 [01:03<20:42,  2.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 1.0383 - accuracy: 0.3945\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.8010\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.1362\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.7026\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 13.2565 - model_275_loss: 0.1568 - model_276_loss: 0.3347 - model_278_loss: 0.3005 - model_277_loss: 0.3764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   3%|▎         | 16/466 [01:05<20:18,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.3833\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.8530\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.2725\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.5127\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 12.9556 - model_275_loss: 0.1667 - model_276_loss: 0.4711 - model_278_loss: 0.2910 - model_277_loss: 0.3706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   4%|▎         | 17/466 [01:08<19:55,  2.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.4026\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.7988\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.3689\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.4380\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 12.2735 - model_275_loss: 0.2153 - model_276_loss: 0.4557 - model_278_loss: 0.2690 - model_277_loss: 0.3353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   4%|▍         | 18/466 [01:11<19:37,  2.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.5317\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.8027\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.3230\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.6160\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 12.1547 - model_275_loss: 0.2215 - model_276_loss: 0.3104 - model_278_loss: 0.2677 - model_277_loss: 0.3402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   4%|▍         | 19/466 [01:13<19:29,  2.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.5396\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.7434\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.2339\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.7214\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 11.6906 - model_275_loss: 0.2377 - model_276_loss: 0.2849 - model_278_loss: 0.2614 - model_277_loss: 0.3123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   4%|▍         | 20/466 [01:16<19:18,  2.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.5581\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.7671\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.2205\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.7202\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 2s 24ms/step - loss: 11.8125 - model_275_loss: 0.2050 - model_276_loss: 0.3186 - model_278_loss: 0.2614 - model_277_loss: 0.3164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Train 0 / 100:   5%|▍         | 21/466 [01:18<19:10,  2.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.5493\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.7710\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.2327\n",
            "Epoch 1/1\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.6101\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-CKtN1I-i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}