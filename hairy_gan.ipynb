{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hairy_gan.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/hairy_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veBIubcTxFl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb_cPA4QnnNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "if not os.path.exists('kaggle.json'):\n",
        "  shutil.copy('/content/drive/My Drive/hairy_gan/kaggle.json', 'kaggle.json')\n",
        "  # !pip install -q kaggle\n",
        "  # files.upload()\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !kaggle datasets download -d jessicali9530/celeba-dataset --force\n",
        "  !unzip celeba-dataset.zip\n",
        "  !mv img_align_celeba celeba-dataset\n",
        "  !mv list_eval_partition.csv celeba-dataset/list_eval_partition.csv\n",
        "  !mv list_landmarks_align_celeba.csv celeba-dataset/list_landmarks_align_celeba.csv\n",
        "  !mv list_attr_celeba.csv celeba-dataset/list_attr_celeba.csv\n",
        "  !mv list_bbox_celeba.csv celeba-dataset/list_bbox_celeba.csv\n",
        "\n",
        "  !mkdir celeba-dataset/train\n",
        "  !mkdir celeba-dataset/validation\n",
        "  !mkdir celeba-dataset/test\n",
        "\n",
        "  partitions_df = pd.read_csv('celeba-dataset/list_eval_partition.csv') # 0 => train, 1 => validation, 2 => test\n",
        "  for i, set_name in enumerate(['train', 'validation', 'test']):\n",
        "    set_ids_df = partitions_df.loc[partitions_df['partition'] == i]['image_id']\n",
        "    set_ids = set_ids_df.tolist()\n",
        "    for id in set_ids:\n",
        "      shutil.copy(os.path.join('celeba-dataset/img_align_celeba', id), os.path.join('celeba-dataset', f'{set_name}', id))\n",
        "\n",
        "  !git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "  !pip install scipy==1.1.0\n",
        "\n",
        "  clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_tQXJCU83oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Embedding\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, save_model\n",
        "\n",
        "import keras.backend as K\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.backend import set_session, clear_session\n",
        "# from tensorflow.python.keras.models import load_model\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "import cv2\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMUoX4ES1Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r celeba-dataset/train_filter\n",
        "!mkdir celeba-dataset/train_filter\n",
        "# extract images of particular class for training\n",
        "num_images_each = 50\n",
        "num_train_images = num_images_each\n",
        "feature = 'Eyeglasses'\n",
        "complete_df = pd.read_csv('celeba-dataset/list_attr_celeba.csv')\n",
        "for img_id in complete_df.loc[complete_df[feature] == 1][:num_images_each].filter(['image_id']).to_numpy():\n",
        "  img_id = img_id[0]\n",
        "  shutil.copy(f'celeba-dataset/train/{img_id}', f'celeba-dataset/train_filter/{img_id}')\n",
        "\n",
        "for img_id in complete_df.loc[complete_df[feature] == -1][:num_images_each].filter(['image_id']).to_numpy():\n",
        "  img_id = img_id[0]\n",
        "  shutil.copy(f'celeba-dataset/train/{img_id}', f'celeba-dataset/train_filter/{img_id}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0YZGHPYQQhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r celeba-dataset/validation_set\n",
        "!mkdir celeba-dataset/validation_set\n",
        "# extract images of particular class for training\n",
        "num_images_each = 20\n",
        "offset = num_train_images\n",
        "num_validation_images = num_images_each\n",
        "feature = 'Eyeglasses'\n",
        "complete_df = pd.read_csv('celeba-dataset/list_attr_celeba.csv')\n",
        "for img_id in complete_df.loc[complete_df[feature] == 1][offset:offset + num_images_each].filter(['image_id']).to_numpy():\n",
        "  img_id = img_id[0]\n",
        "  shutil.copy(f'celeba-dataset/train/{img_id}', f'celeba-dataset/validation_set/{img_id}')\n",
        "\n",
        "for img_id in complete_df.loc[complete_df[feature] == -1][offset:offset + num_images_each].filter(['image_id']).to_numpy():\n",
        "  img_id = img_id[0]\n",
        "  shutil.copy(f'celeba-dataset/train/{img_id}', f'celeba-dataset/validation_set/{img_id}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpW0Uqm7v6y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "        self.complete_df = pd.read_csv('celeba-dataset/list_attr_celeba.csv')\n",
        "        # self.features = ['Bald', 'Bangs', 'Eyeglasses', 'Mustache', 'No_Beard', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Hat']\n",
        "        self.features = ['Eyeglasses']\n",
        "        self.num_attrs = len(self.features)\n",
        "\n",
        "    def load_data(self, dataset_type, batch_size=1, is_testing=False):\n",
        "        data_type = dataset_type\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        attribs = []\n",
        "        \n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "            # get attributes\n",
        "\n",
        "            img_attribs = [(val + 1) // 2 for val in self.complete_df.loc[self.complete_df['image_id'] == os.path.basename(img_path)].filter(items=self.features).to_numpy()[0]]\n",
        "\n",
        "            attribs.append(img_attribs)\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "        attribs = np.array(attribs)\n",
        "\n",
        "        return imgs, attribs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False, is_filter=False):\n",
        "        if is_filter:\n",
        "          data_type = 'train_filter'\n",
        "        elif is_testing:\n",
        "          data_type = 'test'\n",
        "        else:\n",
        "          data_type = 'train'\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(len(path) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        path = np.random.choice(path, total_samples, replace=False)\n",
        "\n",
        "        i = 0\n",
        "        while i < self.n_batches - 1:\n",
        "            batch = path[i*batch_size:(i+1)*batch_size]\n",
        "            imgs = []\n",
        "            attribs = []\n",
        "            for img_path in batch:\n",
        "                img = self.imread(img_path)\n",
        "\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img = np.fliplr(img)\n",
        "\n",
        "                imgs.append(img)\n",
        "\n",
        "                # get attributes\n",
        "\n",
        "                img_attribs = np.array([(val + 1) // 2 for val in self.complete_df.loc[self.complete_df['image_id'] == os.path.basename(img_path)].filter(items=self.features).to_numpy()[0]])\n",
        "\n",
        "                attribs.append(img_attribs)\n",
        "\n",
        "            imgs = np.array(imgs)/127.5 - 1.\n",
        "            attribs = np.array(attribs)\n",
        "\n",
        "            i += 1\n",
        "            if i == self.n_batches - 1:\n",
        "              # reset\n",
        "              path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "              path = np.random.choice(path, total_samples, replace=False)\n",
        "              i = 0\n",
        "\n",
        "            yield imgs, attribs\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwT3pToianO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_encoder(img_shape, num_filters=64, kernel_size=4, strides=2):\n",
        "  def build_conv(x, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  x = build_conv(img, num_filters, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  # x.name = 'encoder_output'\n",
        "\n",
        "  model = Model(img, x, name='encoder')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_embedding(img, label, input_shape, attr_size):\n",
        "  label_embedding = Embedding(2, np.prod(input_shape), input_length=attr_size)(label)\n",
        "  # style_embedding = Embedding(2, np.prod(input_shape), input_length=attr_size)(style)\n",
        "  # label_style_embedding = Add()([label_embedding, style_embedding])\n",
        "  # label_style_embedding = Reshape(input_shape[:-1] + (attr_size * input_shape[-1], ))(label_style_embedding)\n",
        "  # emb_img = Concatenate(axis=-1)([img, label_style_embedding])\n",
        "  label_embedding = Reshape(input_shape[:-1] + (attr_size * input_shape[-1], ))(label_embedding)\n",
        "  emb_img = Concatenate(axis=-1)([img, label_embedding])\n",
        "  return emb_img\n",
        "\n",
        "def build_decoder(latent_space_shape, attr_size, num_filters=64, kernel_size=4, strides=1):\n",
        "  def build_deconv(x, num_filters, kernel_size, strides):\n",
        "    x = UpSampling2D(size=2)(x)\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "  img = Input(shape=latent_space_shape)\n",
        "  label = Input(shape=(attr_size, ), dtype='int32')\n",
        "\n",
        "  emb_img = build_embedding(img, label, latent_space_shape, attr_size)\n",
        "\n",
        "  x = build_deconv(emb_img, num_filters * 16, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 8, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 4, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 2, kernel_size=kernel_size, strides=strides)\n",
        "  x = UpSampling2D(size=2)(x)\n",
        "  x = Conv2D(3, kernel_size=kernel_size, strides=strides, padding='same', activation='tanh')(x)\n",
        "  # x.name = 'decoder_output'\n",
        "\n",
        "  model = Model([img, label], x, name='decoder')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_convnet(img, num_filters=64, kernel_size=4, strides=2):\n",
        "  def build_conv(x, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "  \n",
        "  x = build_conv(img, num_filters, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024)(x)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def disc_loss_fn(y_true, y_pred):\n",
        "  return tf.reduce_mean(y_pred) - tf.reduce_mean(y_true)\n",
        "\n",
        "def build_discriminator(img_shape, optimizer):\n",
        "  img = Input(shape=img_shape)\n",
        "  x = build_convnet(img, num_filters=8)\n",
        "  output = Dense(1, name='disc_output', activation='sigmoid')(x)\n",
        "\n",
        "  disc = Model(img, output, name='discriminator')\n",
        "  disc.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  disc.summary()\n",
        "\n",
        "  return disc\n",
        "\n",
        "def build_classifier(img_shape, attr_size, optimizer):\n",
        "  img = Input(shape=img_shape)\n",
        "  x = build_convnet(img)\n",
        "\n",
        "  output = Dense(attr_size, activation='sigmoid', name='classif_output')(x)\n",
        "\n",
        "  classif = Model(img, output, name='classifier')\n",
        "\n",
        "  classif.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  classif.summary()\n",
        "\n",
        "  return classif\n",
        "\n",
        "def gen_loss_fn(y_true, y_pred):\n",
        "  return - tf.reduce_mean(y_pred)\n",
        "\n",
        "def build_combined_generator(img_shape, attr_size, genc, gdec, classifier, discriminator, optimizer):\n",
        "  classifier.trainable = False\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  x_a = Input(shape=img_shape) # original image\n",
        "  a = Input(shape=(attr_size, )) # original attr\n",
        "  b = Input(shape=(attr_size, )) # requested attr\n",
        "  \n",
        "  z = genc(x_a) # latent space representation of original image\n",
        "  x_b = gdec([z, b]) # image with requested attr\n",
        "\n",
        "  b_hat = classifier(x_b) # guess attributes\n",
        "  valid = discriminator(x_b) # guess real or fake\n",
        "\n",
        "  x_a_hat = gdec([z, a]) # reconstr\n",
        "\n",
        "  combined = Model(\n",
        "      inputs=[x_a, a, b],\n",
        "      outputs=[b_hat, valid, x_a_hat], # second output is adversarial loss\n",
        "      name='combined'\n",
        "  )\n",
        "\n",
        "  combined.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'mae'], loss_weights=[10, 1, 100], optimizer=optimizer)\n",
        "\n",
        "  combined.summary()\n",
        "\n",
        "  return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh62XJlhZSTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(elems):\n",
        "  new_elems = elems.copy()\n",
        "  np.random.shuffle(new_elems)\n",
        "  return new_elems\n",
        "\n",
        "def create_random_attrs(attrs):\n",
        "  # new_attrs = np.ones((attrs.shape))\n",
        "  # count = attrs.shape[0]\n",
        "  # attr_size = attrs[0].size\n",
        "  \n",
        "  new_attrs = np.random.randint(0, 2, size=attrs.shape)\n",
        "  # for r in range(count):\n",
        "  #   for c in range(attr_size):\n",
        "  #     if attrs[r, c] == 1 and new_attrs[r, c] == 0:\n",
        "  #       new_attrs[r, c] = 1\n",
        "  \n",
        "  return new_attrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1S3__xyoSCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier_step(gen_batch, classifier):\n",
        "  imgs, attrs = gen_batch\n",
        "\n",
        "  x_a = imgs\n",
        "  a = attrs\n",
        "\n",
        "  classif_history = classifier.fit(x_a, a)\n",
        "\n",
        "  return classif_history\n",
        "\n",
        "def train_discriminator_step(batch_size, gen_batch, enc, dec, discriminator):\n",
        "  imgs, attrs, new_attrs = gen_batch\n",
        "\n",
        "  x_a = imgs\n",
        "  a = attrs\n",
        "  b = new_attrs\n",
        "\n",
        "  z = enc.predict(x_a)\n",
        "  x_b = dec.predict([z, b])\n",
        "\n",
        "  real = np.ones((len(imgs), 1))\n",
        "  fake = np.zeros((len(imgs), 1))\n",
        "  # disc_loss = discriminator.train_on_batch(x_b, discriminator.predict(x_a))\n",
        "  disc_real_loss = discriminator.train_on_batch(x_a, real)\n",
        "  disc_fake_loss = discriminator.train_on_batch(x_b, fake)\n",
        "  # disc_real_history = discriminator.fit(x_a, real)\n",
        "  # disc_fake_history = discriminator.fit(x_b, fake)\n",
        "\n",
        "  class TempObj():\n",
        "    def __init__(self, value):\n",
        "      self.history = {}\n",
        "      self.history['loss'] = [value]\n",
        "\n",
        "  disc_real_history = TempObj(disc_real_loss)\n",
        "  disc_fake_history = TempObj(disc_fake_loss)\n",
        "\n",
        "  return disc_real_history, disc_fake_history\n",
        "\n",
        "def train_encdec_step(batch_size, gen_batch, combined):\n",
        "  imgs, attrs, new_attrs = gen_batch\n",
        "\n",
        "  real = np.ones((batch_size, 1))\n",
        "  # fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  x_a = imgs\n",
        "  a = attrs\n",
        "  b = new_attrs\n",
        "\n",
        "  g_real_loss = combined.train_on_batch([x_a, a, b], [b, real, x_a])\n",
        "  # g_real_history = combined.fit([x_a, a, b], [b, real, x_a])\n",
        "\n",
        "\n",
        "  class TempObj():\n",
        "    def __init__(self, value):\n",
        "      self.history = {}\n",
        "      self.history['loss'] = [value]\n",
        "\n",
        "  g_real_history = TempObj(g_real_loss)\n",
        "  return g_real_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LiPqnQqm6B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_metrics(metrics, histories):\n",
        "  for history in histories:\n",
        "    for k, v in history.history.items():\n",
        "      if metrics.get(k) is None:\n",
        "        metrics[k] = v\n",
        "      else:\n",
        "        metrics[k].append(v[0]) # array of 1 elem => elem\n",
        "  return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJeIkMXPtnjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_metrics(metrics):\n",
        "  num_plots = len(metrics.keys())\n",
        "\n",
        "  fig, axes = plt.subplots(num_plots)\n",
        "\n",
        "  for pl, (title, values) in enumerate(metrics.items()):\n",
        "    axes[pl].plot(values)\n",
        "    axes[pl].set_title(title)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH3zqRKNPN-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classif_build_conv(x, num_filters, kernel_size, strides):\n",
        "  x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fr2OhG8H3jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tag_name(dic):\n",
        "  name = ''\n",
        "  for key, val in dic.items():\n",
        "    name = name + '_' + key + '_' + str(val)\n",
        "  return name[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbgXa0M9aWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HairyGan(): # based on AttGan\n",
        "  def __init__(self, flags={}):\n",
        "\n",
        "    self.learning_rate = flags['learning_rate']\n",
        "\n",
        "    self.enc_dec_interval = flags['enc_dec_interval']\n",
        "    self.disc_interval = flags['disc_interval']\n",
        "\n",
        "    self.img_rows = flags['orig_dim']\n",
        "    self.img_cols = flags['orig_dim']\n",
        "    self.img_channels = 3\n",
        "\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "    \n",
        "    patch = int(self.img_rows / 2**4)\n",
        "    self.disc_out = (patch, patch, 1) # output shape of discriminator\n",
        "\n",
        "    self.dl = DataLoader(dataset_name='celeba-dataset', img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "    self.optimizer = Adam(learning_rate=self.learning_rate, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    self.flags = flags\n",
        "\n",
        "    if self.flags['from_scratch']:\n",
        "      self.enc = build_encoder(self.img_shape)\n",
        "      self.dec = build_decoder((4, 4, 1024), self.dl.num_attrs)\n",
        "      self.disc = build_discriminator(self.img_shape, self.optimizer)\n",
        "    else:\n",
        "      if os.path.exists(os.path.join(project_path, 'enc.h5')):\n",
        "        print('Loading encoder from file')\n",
        "        self.enc = load_model(os.path.join(project_path, 'enc.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "      else:\n",
        "        self.enc = build_encoder(self.img_shape)\n",
        "      \n",
        "      if os.path.exists(os.path.join(project_path, 'dec.h5')):\n",
        "        print('Loading decoder from file')\n",
        "        self.dec = load_model(os.path.join(project_path, 'dec.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "      else:\n",
        "        self.dec = build_decoder((4, 4, 1024), self.dl.num_attrs)\n",
        "      \n",
        "      # if os.path.exists(os.path.join(project_path, 'disc.weights')):\n",
        "      #   print('Loading disc from file')\n",
        "      #   self.disc.load_weights(os.path.join(project_path, 'disc.weights'))\n",
        "\n",
        "      if os.path.exists(os.path.join(project_path, 'disc.h5')):\n",
        "        print('Loading disc from file')\n",
        "        self.disc = load_model(os.path.join(project_path, 'disc.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "      else:\n",
        "        self.disc = build_discriminator(self.img_shape, self.optimizer)\n",
        "\n",
        "    # if os.path.exists(os.path.join(project_path, 'classif.weights')):\n",
        "    #   print('Loading classif from file')\n",
        "    #   self.classif.load_weights(os.path.join(project_path, 'classif.weights'))\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'classif.h5')):\n",
        "      print('Loading classif from file')\n",
        "      self.classif = load_model(os.path.join(project_path, 'classif.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "    else:\n",
        "      self.classif = build_classifier(self.img_shape, self.dl.num_attrs, self.optimizer)\n",
        "\n",
        "    self.combined = build_combined_generator(self.img_shape, self.dl.num_attrs, self.enc, self.dec, self.classif, self.disc, self.optimizer)   \n",
        "\n",
        "    self.metrics = {}\n",
        "\n",
        "    if flags['new_dim'] != flags['orig_dim']: # progress!\n",
        "      self.progress(flags['new_dim'])\n",
        "\n",
        "  def progress(self, new_dim):\n",
        "    print('Applying progression')\n",
        "\n",
        "    orig_dim = self.img_shape[0]\n",
        "    input = Input(shape=(new_dim, new_dim, 3))\n",
        "    dim_scale = new_dim / orig_dim\n",
        "    num_new_layers = round(math.log(dim_scale, 2))\n",
        "\n",
        "    # update data loader\n",
        "    print('Update data loader')\n",
        "    self.dl = DataLoader(dataset_name='celeba-dataset', img_res=(new_dim, new_dim))\n",
        "\n",
        "    # update classifier\n",
        "    print('Update classifier')\n",
        "    layers = self.classif.layers[2:]\n",
        "    output = classif_build_conv(input, 64, 4, 2)\n",
        "    output = classif_build_conv(output, 64, 4, 2)\n",
        "    for i, layer in enumerate(layers):\n",
        "      output = layer(output)\n",
        "    new_classif = Model(input=input, output=output, name='classifier')\n",
        "    new_classif.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "    self.classif = new_classif\n",
        "\n",
        "  def pretrain_classifier(self, num_epochs, batch_size, visualize_interval):\n",
        "    self.classif.trainable = True\n",
        "\n",
        "    # set up data loader\n",
        "    batch_gen = self.dl.load_batch(batch_size=batch_size, is_filter=self.flags['filter_on'])\n",
        "    for i, elem in enumerate(batch_gen):\n",
        "      break\n",
        "  \n",
        "    num_batches = self.dl.n_batches\n",
        "    steps_per_epoch = num_batches\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {num_epochs}', total=steps_per_epoch):\n",
        "        gen_batch = next(batch_gen)\n",
        "\n",
        "        classif_history = train_classifier_step(gen_batch, self.classif)\n",
        "        classif_history.history['classif_loss'] = classif_history.history.pop('loss')\n",
        "\n",
        "        self.metrics = add_metrics(self.metrics, [classif_history])\n",
        "\n",
        "        clear_output()\n",
        "        if (count + 1) % visualize_interval == 0:\n",
        "          try:\n",
        "            self.sample_images(epoch, step, is_filter=self.flags['filter_on'])\n",
        "            \n",
        "            # save model\n",
        "            save_model(self.classif, os.path.join(project_path, 'classif.h5'))\n",
        "            # self.classif.save_weights('classif.weights')\n",
        "            # shutil.copy('classif.weights', os.path.join(project_path, 'classif.weights'))\n",
        "\n",
        "            # visualize loss/accuracy\n",
        "            visualize_metrics(self.metrics)\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        count += 1\n",
        "\n",
        "  def train(self, num_epochs, batch_size, visualize_interval):\n",
        "    self.classif.trainable = False\n",
        "\n",
        "    # set up data loader\n",
        "    batch_gen = self.dl.load_batch(batch_size=batch_size, is_filter=self.flags['filter_on'])\n",
        "    for i, elem in enumerate(batch_gen):\n",
        "      break\n",
        "  \n",
        "    num_batches = self.dl.n_batches\n",
        "    steps_per_epoch = num_batches\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {num_epochs}', total=steps_per_epoch):\n",
        "        imgs, attrs = next(batch_gen)\n",
        "        new_attrs = create_random_attrs(attrs)\n",
        "        gen_batch = (imgs, attrs, new_attrs)\n",
        "\n",
        "        print('Train discriminator')\n",
        "        self.disc.trainable = True\n",
        "        for i in range(self.disc_interval):\n",
        "          disc_real_history, disc_fake_history = train_discriminator_step(batch_size, gen_batch, self.enc, self.dec, self.disc)\n",
        "\n",
        "        print('Train encoder/decoder')\n",
        "        self.disc.trainable = False\n",
        "        for i in range(self.enc_dec_interval):\n",
        "          g_real_history = train_encdec_step(batch_size, gen_batch, self.combined)\n",
        "\n",
        "        # disc_real_history.history['disc_real_loss'] = disc_real_history.history.pop('loss')\n",
        "        # disc_fake_history.history['disc_fake_loss'] = disc_fake_history.history.pop('loss')\n",
        "        g_real_history.history['g_real_loss'] = g_real_history.history.pop('loss')\n",
        "\n",
        "        # self.metrics = add_metrics(self.metrics, [disc_real_history, disc_fake_history, g_real_history])\n",
        "        self.metrics = add_metrics(self.metrics, [disc_real_history, g_real_history])\n",
        "\n",
        "        # set to trainable again\n",
        "        self.disc.trainable = True\n",
        "\n",
        "        clear_output()\n",
        "        if (count + 1) % visualize_interval == 0:\n",
        "          try:\n",
        "            self.sample_images(epoch, step, is_filter=self.flags['filter_on'])\n",
        "            \n",
        "            # save models\n",
        "            save_model(self.enc, 'enc.h5')\n",
        "            shutil.copy('enc.h5', os.path.join(project_path, 'enc.h5'))\n",
        "            # shutil.copy('enc.h5', os.path.join(project_path, 'backup', 'enc.h5'))\n",
        "\n",
        "            save_model(self.dec, 'dec.h5')\n",
        "            shutil.copy('dec.h5', os.path.join(project_path, 'dec.h5'))\n",
        "            # shutil.copy('dec.h5', os.path.join(project_path, 'backup', 'dec.h5'))\n",
        "\n",
        "            save_model(self.disc, os.path.join(project_path, 'disc.h5'))\n",
        "            # save_model(self.combined, os.path.join(project_path, 'combined.h5'))\n",
        "            # self.disc.save_weights('disc.weights')\n",
        "            # shutil.copy('disc.weights', os.path.join(project_path, 'disc.weights'))\n",
        "            # shutil.copy('disc.weights', os.path.join(project_path, 'backup', 'disc.weights'))\n",
        "\n",
        "            # self.combined.save_weights('combined.weights')\n",
        "            # shutil.copy('combined.weights', os.path.join(project_path, 'combined.weights'))\n",
        "            # shutil.copy('combined.weights', os.path.join(project_path, 'backup', 'combined.weights'))\n",
        "\n",
        "            # visualize loss/accuracy\n",
        "            visualize_metrics(self.metrics)\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "        \n",
        "        count += 1\n",
        "\n",
        "  def sample_images(self, epoch, batch_i, is_filter=False):\n",
        "    print(f'Epoch: {epoch} with batch: {batch_i}')\n",
        "    rows, cols = 2, 3\n",
        "\n",
        "    imgs, attrs = self.dl.load_data('test' if not is_filter else 'train_filter', batch_size=2, is_testing=True)\n",
        "\n",
        "    new_attrs = create_random_attrs(attrs)\n",
        "\n",
        "    encodings = self.enc.predict(imgs)\n",
        "\n",
        "    reconstrs = self.dec.predict([encodings, attrs])\n",
        "\n",
        "    new_imgs = self.dec.predict([encodings, new_attrs])\n",
        "    # combined.predict([imgs, attrs, new_attrs]) \n",
        "\n",
        "    gen_imgs = np.array([imgs[0], new_imgs[0], reconstrs[0], imgs[1], new_imgs[1], reconstrs[1]])\n",
        "\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    titles = ['Original', 'Translated', 'Reconstructed']\n",
        "    fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    print(f'O.G. images fake or real: {self.disc.predict(imgs)}')\n",
        "    print(f'New images fake or real: {self.disc.predict(new_imgs)}')\n",
        "    print(f'Reconstructed images fake or real: {self.disc.predict(reconstrs)}')\n",
        "    print(f'O.G. image attributes: {self.classif.predict(imgs)}')\n",
        "    print(f'New image attributes: {self.classif.predict(new_imgs)}')\n",
        "    print(f'Reconstructed image attributes: {self.classif.predict(reconstrs)}')\n",
        "\n",
        "    for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axes[i, j].imshow(gen_imgs[count])\n",
        "        axes[i, j].set_title(titles[j])\n",
        "        axes[i, j].axis('off')\n",
        "        count += 1\n",
        "\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9Woi3bGjYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag_options = [\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 1, 'disc_interval': 1, 'learning_rate': 0.000005 },\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 5, 'disc_interval': 1, 'learning_rate': 0.000005 },\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 1, 'disc_interval': 1, 'learning_rate': 0.00005 },\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 3, 'disc_interval': 1, 'learning_rate': 0.00005 },\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 1, 'disc_interval': 1, 'learning_rate': 0.0005 },\n",
        "  # { 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 1, 'disc_interval': 1, 'learning_rate': 0.005 },\n",
        "  # { 'filter_on': False, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 1, 'disc_interval': 1, 'learning_rate': 0.000005 },\n",
        "]\n",
        "\n",
        "og_project_path = '/content/drive/My Drive/hairy_gan'\n",
        "classif = load_model(os.path.join(og_project_path, 'classif.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "\n",
        "tags, losses = [], []\n",
        "best_tag, best_loss = None, None\n",
        "\n",
        "for flags in flag_options:\n",
        "  # create tag name\n",
        "  tag = tag_name(flags)\n",
        "\n",
        "  # set project path\n",
        "  project_path = os.path.join(og_project_path, tag)\n",
        "\n",
        "  # create folder\n",
        "  if not os.path.exists(project_path):\n",
        "    os.makedirs(project_path)\n",
        "\n",
        "  # create gan model\n",
        "  gan = HairyGan(flags)\n",
        "  del gan.classif\n",
        "  gan.classif = classif\n",
        "\n",
        "  # # train gan model\n",
        "  # # if not flags['filter_on']:\n",
        "  # #   gan.train(num_epochs=1, batch_size=32, visualize_interval=100)\n",
        "  # # else:\n",
        "  # #   gan.train(num_epochs=160, batch_size=2, visualize_interval=100)\n",
        "\n",
        "  # # calculate loss\n",
        "  # batch_size = num_validation_images\n",
        "  # imgs, attrs = gan.dl.load_data('validation_set', batch_size=batch_size, is_testing=True)\n",
        "  # new_attrs = np.ones(attrs.shape)\n",
        "  # encodings = gan.enc.predict(imgs)\n",
        "  # reconstrs = gan.dec.predict([encodings, attrs])\n",
        "  # new_imgs = gan.dec.predict([encodings, new_attrs])\n",
        "\n",
        "  # real = np.ones(shape=(batch_size,))\n",
        "  # loss = gan.combined.evaluate([imgs, attrs, new_attrs], [new_attrs, real, imgs])[0]\n",
        "\n",
        "  # # save best tag, loss\n",
        "  # if best_loss is None or loss < best_loss:\n",
        "  #   best_tag = tag\n",
        "  #   best_loss = loss\n",
        "\n",
        "  # tags.append(tag)\n",
        "  # losses.append(loss)\n",
        "\n",
        "  # # delete gan model\n",
        "  # del gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFBh_xEHdiKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = '/content/drive/My Drive/hairy_gan'\n",
        "flags = { 'from_scratch': True, 'filter_on': True, 'orig_dim': 128, 'new_dim': 128, 'enc_dec_interval': 3, 'disc_interval': 1, 'learning_rate': 0.0005 }\n",
        "gan = HairyGan(flags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISfvr5WBr7vE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gan.pretrain_classifier(num_epochs=10000, batch_size=2, visualize_interval=100)\n",
        "if not flags['filter_on']:\n",
        "  gan.train(num_epochs=10000, batch_size=32, visualize_interval=100)\n",
        "else:\n",
        "  gan.train(num_epochs=10000, batch_size=2, visualize_interval=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jgo9vYgvvfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rows, cols = 2, 3\n",
        "\n",
        "imgs, attrs = gan.dl.load_data('train_filter', batch_size=2, is_testing=True)\n",
        "\n",
        "new_attrs = np.ones(attrs.shape)\n",
        "# new_attrs = np.zeros(attrs.shape)\n",
        "# new_attrs[:, 2] = 1\n",
        "\n",
        "encodings = gan.enc.predict(imgs)\n",
        "\n",
        "reconstrs = gan.dec.predict([encodings, attrs])\n",
        "\n",
        "new_imgs = gan.dec.predict([encodings, new_attrs])\n",
        "# combined.predict([imgs, attrs, new_attrs]) \n",
        "\n",
        "print(f'O.G. images fake or real: {gan.disc.predict(imgs)}')\n",
        "print(f'New images fake or real: {gan.disc.predict(new_imgs)}')\n",
        "print(f'Reconstructed images fake or real: {gan.disc.predict(reconstrs)}')\n",
        "print(f'O.G. image attributes: {gan.classif.predict(imgs)}')\n",
        "print(f'New image attributes: {gan.classif.predict(new_imgs)}')\n",
        "print(f'Reconstructed image attributes: {gan.classif.predict(reconstrs)}')\n",
        "\n",
        "gen_imgs = np.array([imgs[0], new_imgs[0], reconstrs[0], imgs[1], new_imgs[1], reconstrs[1]])\n",
        "\n",
        "gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "titles = ['Original', 'Translated', 'Reconstructed']\n",
        "fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in range(rows):\n",
        "  for j in range(cols):\n",
        "    axes[i, j].imshow(gen_imgs[count])\n",
        "    axes[i, j].set_title(titles[j])\n",
        "    axes[i, j].axis('off')\n",
        "    count += 1\n",
        "\n",
        "visualize_metrics(gan.metrics)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLwhKNm7C_j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_model(gan.enc, 'enc.h5')\n",
        "# shutil.copy('enc.h5', os.path.join(project_path, 'enc.h5'))\n",
        "# # shutil.copy('enc.h5', os.path.join(project_path, 'backup', 'enc.h5'))\n",
        "\n",
        "# save_model(gan.dec, 'dec.h5')\n",
        "# shutil.copy('dec.h5', os.path.join(project_path, 'dec.h5'))\n",
        "# # shutil.copy('dec.h5', os.path.join(project_path, 'backup', 'dec.h5'))\n",
        "\n",
        "# save_model(gan.disc, os.path.join(project_path, 'disc.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya_6rf4_TNpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = (128, 128, 3)\n",
        "# all_ones = np.ones(img_shape)\n",
        "all_ones = imgs[0]\n",
        "# all_zeros = np.zeros(img_shape)\n",
        "all_zeros = new_imgs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4BFssNfehr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(all_zeros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdbwYWbegpfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def disc_loss_fn2(y_true, y_pred):\n",
        "  return tf.reduce_mean(y_pred) - tf.reduce_mean(y_true)\n",
        "\n",
        "def build_discriminator2(img_shape, optimizer):\n",
        "  def build_conv(x, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  \n",
        "  num_filters = 8\n",
        "  kernel_size = 4\n",
        "  strides = 2\n",
        "\n",
        "  x = build_conv(img, num_filters, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  # x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  # x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  # x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  # x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  x = Flatten()(x)\n",
        "  # x = Flatten()(img)\n",
        "  x = Dense(1024)(x)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "  output = Dense(1, name='disc_output', activation='sigmoid')(x)\n",
        "\n",
        "  disc = Model(img, output, name='discriminator')\n",
        "  disc.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  disc.summary()\n",
        "\n",
        "  return disc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2JR9i6ohZEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # del disc\n",
        "# disc = build_discriminator2(img_shape, Adam(5e-04))\n",
        "# clear_output()\n",
        "# disc.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3cR8xkl7Ae_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  # loss = disc.train_on_batch(np.array([imgs[0], imgs[1], new_imgs[0], new_imgs[1]]), [1, 1, 0, 0])\n",
        "  loss = disc.train_on_batch(np.array([all_ones, all_zeros]), [1, 0])\n",
        "  # loss = disc.train_on_batch(np.expand_dims(all_ones, axis=0), [1]) # dont uncomment this; this overfits\n",
        "  clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geoTAfguHJ9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'O.G. images fake or real: {disc.predict(np.array([all_ones]))}')\n",
        "print(f'New images fake or real: {disc.predict(np.array([all_zeros]))}')\n",
        "# print(f'O.G. images fake or real: {disc.predict(imgs)}')\n",
        "# print(f'New images fake or real: {disc.predict(new_imgs)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBBTwZTDF9Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}