{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hairy_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PFgBE9Rkd8fOWpAi2lsgE1RrQYJKfdKa",
      "authorship_tag": "ABX9TyNyXndGeosXaDTb8KT2FnRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/hairy_gan/blob/master/hairy_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb_cPA4QnnNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "if not os.path.exists('kaggle.json'):\n",
        "  shutil.copy('/content/drive/My Drive/hairy_gan/kaggle.json', 'kaggle.json')\n",
        "  # !pip install -q kaggle\n",
        "  # files.upload()\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !kaggle datasets download -d jessicali9530/celeba-dataset --force\n",
        "  !unzip celeba-dataset.zip\n",
        "  !mv img_align_celeba celeba-dataset\n",
        "  !mv list_eval_partition.csv celeba-dataset/list_eval_partition.csv\n",
        "  !mv list_landmarks_align_celeba.csv celeba-dataset/list_landmarks_align_celeba.csv\n",
        "  !mv list_attr_celeba.csv celeba-dataset/list_attr_celeba.csv\n",
        "  !mv list_bbox_celeba.csv celeba-dataset/list_bbox_celeba.csv\n",
        "\n",
        "  !mkdir celeba-dataset/train\n",
        "  !mkdir celeba-dataset/validation\n",
        "  !mkdir celeba-dataset/test\n",
        "\n",
        "  partitions_df = pd.read_csv('celeba-dataset/list_eval_partition.csv') # 0 => train, 1 => validation, 2 => test\n",
        "  for i, set_name in enumerate(['train', 'validation', 'test']):\n",
        "    set_ids_df = partitions_df.loc[partitions_df['partition'] == i]['image_id']\n",
        "    set_ids = set_ids_df.tolist()\n",
        "    for id in set_ids:\n",
        "      shutil.copy(os.path.join('celeba-dataset/img_align_celeba', id), os.path.join('celeba-dataset', f'{set_name}', id))\n",
        "\n",
        "  !git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "\n",
        "  !pip install scipy==1.1.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_tQXJCU83oV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6e04ad33-d402-4ef9-c786-bcad0ab77dbf"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Embedding\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, save_model\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.backend import set_session, clear_session\n",
        "# from tensorflow.python.keras.models import load_model\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpW0Uqm7v6y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataLoader():\n",
        "    def __init__(self, dataset_name, img_res):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.img_res = img_res\n",
        "        self.complete_df = pd.read_csv('celeba-dataset/list_attr_celeba.csv')\n",
        "        self.features = ['Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Bushy_Eyebrows', 'Eyeglasses', 'Gender', 'Mouth_Open', 'Mustache', 'No_Beard', 'Pale_Skin', 'Age']\n",
        "        self.num_attrs = 9 # should equal to length of self.features\n",
        "\n",
        "    def load_data(self, dataset_type, batch_size=1, is_testing=False):\n",
        "        data_type = dataset_type\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        batch_images = np.random.choice(path, size=batch_size)\n",
        "\n",
        "        imgs = []\n",
        "        attribs = []\n",
        "        \n",
        "        for img_path in batch_images:\n",
        "            img = self.imread(img_path)\n",
        "            if not is_testing:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if np.random.random() > 0.5:\n",
        "                    img = np.fliplr(img)\n",
        "            else:\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "            imgs.append(img)\n",
        "\n",
        "            # get attributes\n",
        "\n",
        "            img_attribs = [(val + 1) // 2 for val in self.complete_df.loc[self.complete_df['image_id'] == os.path.basename(img_path)].filter(items=self.features).to_numpy()[0][1:].tolist()]\n",
        "\n",
        "            attribs.append(np.array(img_attribs))\n",
        "\n",
        "        imgs = np.array(imgs)/127.5 - 1.\n",
        "        attribs = np.array(attribs)\n",
        "\n",
        "        return imgs, attribs\n",
        "\n",
        "    def load_batch(self, batch_size=1, is_testing=False):\n",
        "        data_type = \"train\" if not is_testing else \"val\"\n",
        "        path = glob('%s/%s/*' % (self.dataset_name, data_type))\n",
        "\n",
        "        self.n_batches = int(len(path) / batch_size)\n",
        "        total_samples = self.n_batches * batch_size\n",
        "\n",
        "        path = np.random.choice(path, total_samples, replace=False)\n",
        "\n",
        "        for i in range(self.n_batches-1):\n",
        "            batch = path[i*batch_size:(i+1)*batch_size]\n",
        "            imgs = []\n",
        "            attribs = []\n",
        "            for img_path in batch:\n",
        "                img = self.imread(img_path)\n",
        "\n",
        "                img = scipy.misc.imresize(img, self.img_res)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        img = np.fliplr(img)\n",
        "\n",
        "                imgs.append(img)\n",
        "\n",
        "                # get attributes\n",
        "\n",
        "                img_attribs = [(val + 1) // 2 for val in self.complete_df.loc[self.complete_df['image_id'] == os.path.basename(img_path)].filter(items=self.features).to_numpy()[0][1:].tolist()]\n",
        "\n",
        "                attribs.append(np.array(img_attribs))\n",
        "\n",
        "            imgs = np.array(imgs)/127.5 - 1.\n",
        "            attribs = np.array(attribs)\n",
        "\n",
        "            yield imgs, attribs\n",
        "\n",
        "    def imread(self, path):\n",
        "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwT3pToianO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_encoder(img_shape, num_filters=64, kernel_size=4, strides=2):\n",
        "  def build_conv(x, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "  img = Input(shape=img_shape)\n",
        "  x = build_conv(img, num_filters, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  x.name = 'encoder_output'\n",
        "\n",
        "  model = Model(img, x, name='encoder')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_embedding(img, label, input_shape, attr_size):\n",
        "  label_embedding = Embedding(2, np.prod(input_shape), input_length=attr_size)(label)\n",
        "  # style_embedding = Embedding(2, np.prod(input_shape), input_length=attr_size)(style)\n",
        "  # label_style_embedding = Add()([label_embedding, style_embedding])\n",
        "  # label_style_embedding = Reshape(input_shape[:-1] + (attr_size * input_shape[-1], ))(label_style_embedding)\n",
        "  # emb_img = Concatenate(axis=-1)([img, label_style_embedding])\n",
        "  label_embedding = Reshape(input_shape[:-1] + (attr_size * input_shape[-1], ))(label_embedding)\n",
        "  emb_img = Concatenate(axis=-1)([img, label_embedding])\n",
        "  return emb_img\n",
        "\n",
        "def build_decoder(latent_space_shape, attr_size, num_filters=64, kernel_size=4, strides=1):\n",
        "  def build_deconv(x, num_filters, kernel_size, strides):\n",
        "    x = UpSampling2D(size=2)(x)\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "  img = Input(shape=latent_space_shape)\n",
        "  label = Input(shape=(attr_size, ), dtype='int32')\n",
        "\n",
        "  emb_img = build_embedding(img, label, latent_space_shape, attr_size)\n",
        "\n",
        "  x = build_deconv(emb_img, num_filters * 16, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 8, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 4, kernel_size=kernel_size, strides=strides)\n",
        "  x = build_deconv(x, num_filters * 2, kernel_size=kernel_size, strides=strides)\n",
        "  x = UpSampling2D(size=2)(x)\n",
        "  x = Conv2D(3, kernel_size=kernel_size, strides=strides, padding='same', activation='tanh')(x)\n",
        "  x.name = 'decoder_output'\n",
        "\n",
        "  model = Model([img, label], x, name='decoder')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_convnet(img, num_filters=64, kernel_size=4, strides=2):\n",
        "  def build_conv(x, num_filters, kernel_size, strides):\n",
        "    x = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "  \n",
        "  x = build_conv(img, num_filters, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 2, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 4, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 8, kernel_size, strides)\n",
        "  x = build_conv(x, num_filters * 16, kernel_size, strides)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1024)(x)\n",
        "  x = InstanceNormalization()(x)\n",
        "  x = LeakyReLU()(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def build_dc(img_shape, attr_size, optimizer): # NOTE: we ignore inputting original image to discriminator head. why? cause im not sure if its important\n",
        "  img = Input(shape=img_shape)\n",
        "  label = Input(shape=(attr_size, ), dtype='int32') # I don't understand. why do we have this?\n",
        "\n",
        "  # emb_img = build_embedding(img, label, img_shape, attr_size)\n",
        "  # x = build_convnet(emb_img)\n",
        "  x = build_convnet(img)\n",
        "  disc_output = Dense(1, name='disc_output')(x)\n",
        "  classif_output = Dense(attr_size, activation='sigmoid', name='classif_output')(x)\n",
        "\n",
        "  dc = Model([img, label], [disc_output, classif_output], name='dc')\n",
        "\n",
        "  dc.compile(loss=['binary_crossentropy', 'binary_crossentropy'], loss_weights=[1, 1], optimizer=optimizer)\n",
        "\n",
        "  dc.summary()\n",
        "\n",
        "  return dc\n",
        "\n",
        "def build_combined_generator(img_shape, attr_size, genc, gdec, dc, optimizer):\n",
        "  dc.trainable = False\n",
        "\n",
        "  x_a = Input(shape=img_shape) # original image\n",
        "  a = Input(shape=(attr_size, )) # original attr\n",
        "  b = Input(shape=(attr_size, )) # requested attr\n",
        "  \n",
        "  z = genc(x_a) # latent space representation of original image\n",
        "  x_b_hat = gdec([z, b]) # image with requested attr\n",
        "\n",
        "  valid, b_hat = dc([x_b_hat, b]) # guess real or fake and guess the requested features \n",
        "\n",
        "  x_a_hat = gdec([z, a]) # reconstr\n",
        "\n",
        "  combined = Model(\n",
        "      inputs=[x_a, a, b],\n",
        "      outputs=[b_hat, valid, x_a_hat],\n",
        "      name='combined'\n",
        "  )\n",
        "\n",
        "  combined.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'mae'], loss_weights=[10, 1, 100], optimizer=optimizer)\n",
        "\n",
        "  combined.summary()\n",
        "\n",
        "  return combined"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh62XJlhZSTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle(elems):\n",
        "  new_elems = elems.copy()\n",
        "  np.random.shuffle(new_elems)\n",
        "  return new_elems\n",
        "\n",
        "def random_attrs(attr_size, count):\n",
        "  return np.random.randint(0, 2, size=(count, attr_size))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1S3__xyoSCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_dc_step(batch_size, batch_gen, genc, gdec, dc):\n",
        "  imgs, attrs = next(batch_gen)\n",
        "  new_attrs = random_attrs(attrs[0].size, batch_size)\n",
        "\n",
        "  real = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  x_a = imgs\n",
        "  a = attrs\n",
        "  b = new_attrs\n",
        "\n",
        "  z = genc.predict(x_a)\n",
        "  x_b_hat = gdec.predict([z, b])\n",
        "\n",
        "  dc_real_history = dc.fit([x_a, a], [real, a])\n",
        "  dc_fake_history = dc.fit([x_b_hat, b], [fake, b])\n",
        "\n",
        "  return dc_real_history, dc_fake_history\n",
        "\n",
        "def train_encdec_step(batch_size, batch_gen, combined):\n",
        "  imgs, attrs = next(batch_gen)\n",
        "  new_attrs = random_attrs(attrs[0].size, batch_size)\n",
        "\n",
        "  real = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size, 1))\n",
        "\n",
        "  x_a = imgs\n",
        "  a = attrs\n",
        "  b = new_attrs\n",
        "\n",
        "  g_real_history = combined.fit([x_a, a, b], [b, fake, x_a])\n",
        "\n",
        "  return g_real_history"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSbgXa0M9aWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HairyGan(): # based on AttGan\n",
        "  def __init__(self):\n",
        "\n",
        "    self.img_rows = 128\n",
        "    self.img_cols = 128\n",
        "    self.img_channels = 3\n",
        "\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
        "    \n",
        "    patch = int(self.img_rows / 2**4)\n",
        "    self.disc_out = (patch, patch, 1) # output shape of discriminator\n",
        "\n",
        "    self.dl = DataLoader(dataset_name='celeba-dataset', img_res=(self.img_rows, self.img_cols))\n",
        "\n",
        "    self.optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    if os.path.exists(os.path.join(project_path, 'enc.h5')):\n",
        "      self.enc = load_model(os.path.join(project_path, 'enc.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "    else:\n",
        "      self.enc = build_encoder(self.img_shape)\n",
        "    \n",
        "    if os.path.exists(os.path.join(project_path, 'dec.h5')):\n",
        "      self.dec = load_model(os.path.join(project_path, 'dec.h5'), custom_objects={'InstanceNormalization': InstanceNormalization})\n",
        "    else:\n",
        "      self.dec = build_decoder((4, 4, 1024), self.dl.num_attrs)\n",
        "\n",
        "    self.dc = build_dc(self.img_shape, self.dl.num_attrs, self.optimizer)\n",
        "    \n",
        "    if os.path.exists(os.path.join(project_path, 'dc.weights')):\n",
        "      self.dc.load_weights(os.path.join(project_path, 'dc.weights'))\n",
        "\n",
        "    self.combined = build_combined_generator(self.img_shape, self.dl.num_attrs, self.enc, self.dec, self.dc, self.optimizer)   \n",
        "\n",
        "    self.metrics = {}\n",
        "\n",
        "  def train(self, num_epochs, batch_size, sample_interval):\n",
        "    # set up data loader\n",
        "    gen = self.dl.load_batch(batch_size=batch_size)\n",
        "    for i, elem in enumerate(gen):\n",
        "      break\n",
        "\n",
        "    self._train(num_epochs=100, num_batches=self.dl.n_batches, batch_size=batch_size, batch_gen=gen, sample_interval=sample_interval)\n",
        "\n",
        "  def _train(self, num_epochs, num_batches, batch_size, batch_gen, sample_interval):\n",
        "    steps_per_epoch = num_batches\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {num_epochs}', total=steps_per_epoch):\n",
        "        dc_real_history, dc_fake_history = train_dc_step(batch_size, batch_gen, self.enc, self.dec, self.dc)\n",
        "        g_real_history = train_encdec_step(batch_size, batch_gen, self.combined)\n",
        "\n",
        "        dc_real_history.history['dc_real_loss'] = dc_real_history.history.pop('loss')\n",
        "        dc_fake_history.history['dc_fake_loss'] = dc_fake_history.history.pop('loss')\n",
        "        g_real_history.history['g_real_loss'] = g_real_history.history.pop('loss')\n",
        "\n",
        "        self.metrics = add_metrics(self.metrics, [dc_real_history, dc_fake_history, g_real_history])\n",
        "\n",
        "        if (step + 1) % sample_interval == 0:\n",
        "          self.sample_images(epoch, step)\n",
        "          \n",
        "          # save models\n",
        "          save_model(self.enc, 'enc.h5')\n",
        "          shutil.copy('enc.h5', os.path.join(project_path, 'enc.h5'))\n",
        "\n",
        "          save_model(self.dec, 'dec.h5')\n",
        "          shutil.copy('dec.h5', os.path.join(project_path, 'dec.h5'))\n",
        "\n",
        "          self.dc.save_weights('dc.weights')\n",
        "          shutil.copy('dc.weights', os.path.join(project_path, 'dc.weights'))\n",
        "\n",
        "          self.combined.save_weights('combined.weights')\n",
        "          shutil.copy('combined.weights', os.path.join(project_path, 'combined.weights'))\n",
        "\n",
        "          # visualize loss/accuracy\n",
        "          visualize_metrics(self.metrics)\n",
        "\n",
        "  def sample_images(self, epoch, batch_i):\n",
        "    print(f'Epoch: {epoch} with batch: {batch_i}')\n",
        "    rows, cols = 2, 3\n",
        "\n",
        "    imgs, attrs = self.dl.load_data('test', batch_size=2, is_testing=True)\n",
        "\n",
        "    new_attrs = random_attrs(attrs[0].size, attrs.shape[0])\n",
        "\n",
        "    encodings = self.enc.predict(imgs)\n",
        "\n",
        "    reconstrs = self.dec.predict([encodings, attrs])\n",
        "\n",
        "    new_imgs = self.dec.predict([encodings, new_attrs])\n",
        "    # combined.predict([imgs, attrs, new_attrs]) \n",
        "\n",
        "    gen_imgs = np.array([imgs[0], new_imgs[0], reconstrs[0], imgs[1], new_imgs[1], reconstrs[1]])\n",
        "\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    titles = ['Original', 'Translated', 'Reconstructed']\n",
        "    fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axes[i, j].imshow(gen_imgs[count])\n",
        "        axes[i, j].set_title(titles[j])\n",
        "        axes[i, j].axis('off')\n",
        "        count += 1\n",
        "\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LiPqnQqm6B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_metrics(metrics, histories):\n",
        "  for history in histories:\n",
        "    for k, v in history.history.items():\n",
        "      if metrics.get(k) is None:\n",
        "        metrics[k] = v\n",
        "      else:\n",
        "        metrics[k].append(v[0]) # array of 1 elem => elem\n",
        "  return metrics"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJeIkMXPtnjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_metrics(metrics):\n",
        "  num_plots = len(metrics.keys())\n",
        "\n",
        "  fig, axes = plt.subplots(num_plots)\n",
        "\n",
        "  for pl, (title, values) in enumerate(metrics.items()):\n",
        "    axes[pl].plot(values)\n",
        "    axes[pl].set_title(title)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCeFNls66FAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "340697e0-0ccc-4222-bc21-4ce303f213aa"
      },
      "source": [
        "project_path = '/content/drive/My Drive/hairy_gan'\n",
        "gan = HairyGan()\n",
        "gan.train(num_epochs=2, batch_size=32, sample_interval=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"dc\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 64)   3136        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_1 (Insta (None, 64, 64, 64)   2           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 64)   0           instance_normalization_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 128)  131200      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_2 (Insta (None, 32, 32, 128)  2           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 128)  0           instance_normalization_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 256)  524544      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_3 (Insta (None, 16, 16, 256)  2           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 256)  0           instance_normalization_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 8, 8, 512)    2097664     leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_4 (Insta (None, 8, 8, 512)    2           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 8, 8, 512)    0           instance_normalization_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 4, 4, 1024)   8389632     leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_5 (Insta (None, 4, 4, 1024)   2           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 4, 4, 1024)   0           instance_normalization_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 16384)        0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         16778240    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "instance_normalization_6 (Insta (None, 1024)         2           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 1024)         0           instance_normalization_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "disc_output (Dense)             (None, 1)            1025        leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "classif_output (Dense)          (None, 9)            9225        leaky_re_lu_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 27,934,678\n",
            "Trainable params: 27,934,678\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"combined\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 4, 4, 1024)   11154112    input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 9)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, 128, 128, 3)  178830723   model_1[1][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 model_1[1][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 9)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dc (Model)                      [(None, 1), (None, 9 27934678    model_2[1][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 217,919,513\n",
            "Trainable params: 189,977,027\n",
            "Non-trainable params: 27,942,486\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "Train 0 / 100:   0%|          | 0/5086 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 15.8611 - disc_output_loss: 15.4249 - classif_output_loss: 0.4361\n",
            "Epoch 1/1\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.6183 - disc_output_loss: 0.0000e+00 - classif_output_loss: 0.6183\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-38d25c02de67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mproject_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/hairy_gan'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHairyGan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-8ac4f5ae802c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8ac4f5ae802c>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, num_epochs, num_batches, batch_size, batch_gen, sample_interval)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Train {epoch} / {num_epochs}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdc_real_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_fake_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dc_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mg_real_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_encdec_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mdc_real_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dc_real_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_real_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-29976e8f8e74>\u001b[0m in \u001b[0;36mtrain_encdec_step\u001b[0;34m(batch_size, batch_gen, combined)\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mg_real_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mg_real_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3632\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "963yDk9Qif1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "9ccebb66-ef36-487d-c6d8-dcad2cdf1402"
      },
      "source": [
        "# gan.sample_images(1, 1)\n",
        "batch_size = 2\n",
        "imgs, attrs = gan.dl.load_data('test', batch_size=batch_size, is_testing=True)\n",
        "history = gan.dc.fit([imgs, attrs], [np.ones((batch_size, 1)), attrs])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``imageio.imread`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "2/2 [==============================] - 1s 337ms/step - loss: 15.7385 - disc_output_loss: 15.4249 - classif_output_loss: 0.3135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhyg480onlG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "398f1b82-7f2f-4845-be31-bb613a2f9cef"
      },
      "source": [
        "# metrics = {}\n",
        "# history.history['dc_loss'] = history.history.pop('loss')\n",
        "add_metrics(metrics, [history])\n",
        "metrics"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classif_output_loss': [0.31351757, 0.31351757, 0.31351757],\n",
              " 'dc_loss': [15.738466262817383, 15.738466262817383, 15.738466262817383],\n",
              " 'disc_output_loss': [15.424949, 15.424949, 15.424949]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAAp2mh_2WTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "31dde0e0-fa37-478a-9a45-0c515e7efae9"
      },
      "source": [
        "visualize_metrics(metrics)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Ddb2Ua2b5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "775817c5-0609-4444-a519-1c4286f61caa"
      },
      "source": [
        "metrics['dc_loss']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15.738466262817383, 15.738466262817383, 15.738466262817383]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycFOPSS2v4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}